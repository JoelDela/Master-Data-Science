{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces the concept of Convolutional Neural Networks (CNN or ConvNets)(LeCun, 1989), one of the most important neural networks architecture that have been succesfully applied to solve many grid-like data based problems, such the ones related with image processing (2D grid of pixels) --image recognition, edge detection, etc-- or the analysis of time series (1D grid).\n",
    "\n",
    "CNNs were inspired by how the visual cortex of human brain works when recognizing objects.\n",
    "Our vision is based on multiple cortex levels, each one recognizing more and\n",
    "more structured information. First, we see single pixels; then we can recognize\n",
    "simple geometric forms. And then... more and more sophisticated elements such as objects and so on.\n",
    "In the same way, CNNs learn meaningful representations of the input data in a compositional and hierarchical way -- any abstract concept is based on simpler concepts!\n",
    "\n",
    "To do that, CNNs make use of three main mechanisms:\n",
    "- **Sparse interactions**. CNNs, which replace at least one of the fully connected layers of a neural network by the convolution operation, have less capacity than a fully connected network. As we need to store fewer parameters, the training is more efficient \n",
    "\n",
    "- **Parameter sharing**. This means that rather than learning a separate set of parameters for every location, we learn only one set.\n",
    "\n",
    "- **Equivariant representation**. As a consequence of parameter sharing, CNNs have the ability to detect the same feature independently from the location where it is placed in the input image\n",
    "\n",
    "In this class, we will dive deep in all these concepts, covering the following topics:\n",
    "\n",
    "- Understanding convolution operations. \n",
    "\n",
    "- Learning about the building blocks of CNN architectures\n",
    "\n",
    "- Implementing deep convolutional neural networks in keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvNets are simply neural networks that use the convolutional operation in place of general matrix multiplication in at least one of their layers. As we will see in the next section, convolutions aggregate information from a neighborhood and act as a pattern-matching filters, activating more strongly in response to specific local patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the convolution operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start developing intuition for the convolution operation in one dimension. Given two functions $x(t)$ and $\\omega(t)$, the (discrete) convolution operation is defined as\n",
    "$$ s( t ) = (x\\ast\\omega)(t) = \\sum_a x ( a ) \\omega ( t - a )$$\n",
    "\n",
    "An equivalent form of this operation given the commutativity of the convolution operation is as follows:\n",
    "$$ s( t ) = \\sum_a x ( t - a ) \\omega ( a )$$\n",
    "\n",
    "Furthermore, the negative sign (flipping) can be replaced to get cross-correlation given as follows:\n",
    "$$ s ( t ) = \\sum_a x ( t + a ) \\omega ( a )$$\n",
    "*In the machine learning community, both operators (convolutions and cross-correlations) are used interchangeably.*\n",
    "\n",
    "From the above equations, convolutions can be seen as a weighted sum of $x(t)$, with weights controlled by the $\\omega(t)$ function -- weights depends on the distance between any $t$ and the output point. That is, the output point summarizes information about the value of $x(t)$ across the function’s full domain, but it does so according to how far away these points are from it. \n",
    "\n",
    "![Alt text](../imgs/13_CNN_convgaus.gif)\n",
    "\n",
    "*The green curve shows the convolution of the blue and red curves as a function of t, the position indicated by the vertical green line. The gray region indicates the product g(tau)f(t-tau) as a function of t, so its area as a function of t is precisely the convolution.*\n",
    "\n",
    "Formally, a convolution is an operation on two real functions. The first argument $x$ is referred as the input, and the second argument $\\omega$ as the kernel. The output is referred as the feature map. In two dimensions, we have the following expression::\n",
    "\n",
    "$$ S(i, j) = (X\\ast W)(i, j) = \\sum_m \\sum_n X(m,n) W (i-m, j-n)$$\n",
    "\n",
    "![Alt text](../imgs/13_CNN_convolution.gif)\n",
    "\n",
    "*Note that convolution is equivalent to converting both the input and the kernel to the frequency domain using a Fourier transform, performing point-wise multiplication of the two signals, and converting back to the time domain using an inverseFourier transform.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex: Define a function that implements the convolutional operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mtx = np.array([[1,2,4],[2,1,0]])\n",
    "r,c = mtx.shape\n",
    "mtx[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1//4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx_input[:-2,:-1] * mtx_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convolve2d(input_, kernel): \n",
    "    \n",
    "    rows_input, cols_input = input_.shape\n",
    "    rows_kernel, cols_kernel = kernel.shape\n",
    "    cols_out = cols_input - cols_kernel\n",
    "    rows_out = rows_input - rows_kernel\n",
    "    mtx = np.zeros(shape=(rows_out, cols_out))\n",
    "    \n",
    "    print(mtx.shape)\n",
    "    \n",
    "    for i,row in enumerate(range(0,rows_out)):\n",
    "        for j,col in enumerate(range(0,cols_out)):\n",
    "            \n",
    "            mtx_aux = input_[row:row+rows_kernel, col:cols_kernel]\n",
    "            \n",
    "            assert mtx_aux.shape == kernel.shape, 'Different shapes for mtx_aux and kernel'\n",
    "                   \n",
    "            mtx[row,col] = np.sum(mtx_aux * kernel)\n",
    "            \n",
    "    return mtx\n",
    "\n",
    "def convolve2d_stride(input_, kernel, stride):\n",
    "    input_shape = input_.shape\n",
    "    kernel_shape = kernel.shape\n",
    "    output_shape = np.array([input_shape[0] - kernel_shape[0],\n",
    "                    input_shape[1] - kernel_shape[1]]) // stride \n",
    "    output_shape = output_shape + 1 if stride > 1 else output_shape\n",
    "    output = np.zeros(output_shape)  \n",
    "    for i, x in enumerate(range(0, input_shape[0] - kernel_shape[0], stride)):     \n",
    "        for j, y in enumerate(range(0, input_shape[1] - kernel_shape[1], stride)):\n",
    "            m = input_[x:x + kernel_shape[0], \n",
    "                       y:y + kernel_shape[1]]\n",
    "            output[i, j] = np.sum(m * kernel)        \n",
    "    return output\n",
    "\n",
    "mtx_input = np.array([[1,2,3,4],[1,4,2,3],[3,3,4,0],[0,0,3,4]])\n",
    "mtx_kernel = np.array([[1,2],[2,3]])\n",
    "\n",
    "convolve2d_stride(mtx_input, mtx_kernel,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, color\n",
    "from skimage import exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    # Load the image\n",
    "    img = io.imread(path)    \n",
    "    # Convert the image to grayscale (1 channel)\n",
    "    img = color.rgb2gray(img)      \n",
    "    return img\n",
    "\n",
    "# load_image('./training_input/63400.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the convolution. What happens with the figure? Try with other kernels (`np.array([[0,-1,0],[-1,5,-1],[0,-1,0]])`, for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def run(image_path, stride, kernel=np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]])):\n",
    "    img = load_image(image_path)\n",
    "    edges = convolve2d_stride(img, kernel, stride)\n",
    "    # Adjust the contrast of the filtered image by applying Histogram Equalization\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        edges_equalized = exposure.equalize_adapthist(edges/np.max(np.abs(edges)), clip_limit=0.03)\n",
    "        plt.imshow(edges_equalized, cmap=plt.cm.gray)    \n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "run('./training_input/63400.jpg', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run with kernel np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run with kernel np.array([[1,1,1],[1,1,1],[1,1,1]])/9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run with kernel np.array([[0,-1,0],[-1,5,-1],[0,-1,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variants of the basic Convolution Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Striding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we could be interested in downsampling the output of the convolution function. In such a case, the computational cost associated with the convolutional operation would be reduced at the expense of not extracting the feature as finely as possible.\n",
    "\n",
    "One way of accomplishing this is by using a stride $s$. The stride means that we are going to sample only every $s$ pixels in each direction in the output:\n",
    "\n",
    "$$ S(i, j) = (X\\ast W)(i, j) = \\sum_m \\sum_n X_{(i-1)s + m, (j-1)s + n} W_{m, n}$$\n",
    "\n",
    "![Alt text](../imgs/13_CNN_striding.gif) \n",
    "\n",
    "\n",
    "For instance, a stride of 2 means picking slides 2 pixels apart, skipping every other slide in the process, downsizing by roughly a factor of 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex. Change the convolution function to include the stride parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve2d_stride(input_, kernel, stride): \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stride(image_path, kernel=np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]]), stride=2):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_stride, with stride = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_stride, with stride = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Have a look at the following convolution animation:\n",
    "\n",
    "![Alt text](../imgs/13_CNN_conv_.gif)\n",
    "\n",
    "What happens with the dimension of the feature matrix?\n",
    "Yes, she is reduced from a 5×5 dimension to a 3×3 one. This isn’t ideal, as often we’d like the size of the output to equal the input.\n",
    " \n",
    "![Alt text](../imgs/13_CNN_padding.gif) \n",
    " \n",
    " \n",
    "Padding allows us to solve this problem easily. In essence, this technique pad the edges with extra, “fake” pixels (usually of value 0). \n",
    "In other words, padding defines how the border of a sample is handled. A padded convolution will keep the spatial output dimensions equal to the input, whereas unpadded convolutions will crop away some of the borders if the kernel is larger than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex. Add padding funcionality to the prevoius convolve2d_stride function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve2d_stride_padding(input_, kernel, stride, padding):   \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stride_padding(image_path, kernel=np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]]), \n",
    "                       stride=2, padding=2):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_stride_padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Blocks of CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the input layer, Convnets mainly consist of stacks of convolution and max-pooling layers. This two elements are responsible for extracting features from the input and ususally repeats as a block several times to increase the depth of a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it said before, convolutional layers are the key feature layers that makes CNNs different from other ordinary neural networks, and are characterized for applying a convolution operation to the input, passing the result (after activated) to the next layer. In keras, we call a convolutional layer as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: In Keras there are two options for padding: same or valid. Same means we pad with the number on the edge and valid means no padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Pooling operation replaces the output of the net at a certain location with a summary statistic of the nearby outputs. \n",
    "As a consequence, pooling helps to make the representation approximately invariant to small translations of the input. \n",
    "\n",
    "Mainly there are two types of pooling:\n",
    "\n",
    "- **max-pooling**: returns the maximum output within a rectangular neighborhood. In keras we invoke max-pooling using the next command:\n",
    "```\n",
    "keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)\n",
    "```\n",
    "\n",
    "- **average-pooling**: computes the average of a neighborhood. Keras implements average pooling in the function:\n",
    "```\n",
    "keras.layers.AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)\n",
    "```\n",
    "\n",
    "![Alt text](../imgs/13_CNN_pooling_layer_.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing CNNs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the previous ideas in mind, let's practice and play around with CNNs to solve some common images' problems. Let's start with MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As all of you probably know, this data set consists of handwritten digits (60,000 training examples and 10,000 test\n",
    "examples). The task at hand is to predict the digit given the image, so this is a multiclassifcation problem with ten classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex. Import and load the data using `keras.datasets.mnist.load_data()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#MNIST\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#load X_train, y_train, X_test, y_test and plot 4 images using plt.imshow()\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a one-hot encoding and normalize the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize X_train, X_test\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "\n",
    "# Convert Ytrain, Y_test to one-hot . Use np_utils.to_categorical\n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a CNN following these main points:\n",
    "1.\tnetwork structure: two convolution (RELU) + a max pooling layer + two layer fully connected\n",
    "2.\tkernel' size = 3 × 3.\n",
    "3.\tThe pooling operation is done over sections of dimensionality 2 × 2.\n",
    "4.  Set dropout parameter to 0.25\n",
    "5.\tApply softmax in the output layer; relu in other case\n",
    "6.  Set the loss function as categorical entropy\n",
    "7.\tUse adadelta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               692352    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 693,962\n",
      "Trainable params: 693,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "#CNN\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, input_shape=(28,28,1), activation='relu', kernel_size=(3,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 18s 308us/step - loss: 0.0100\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 18s 306us/step - loss: 0.0076\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 19s 322us/step - loss: 0.0061\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 19s 318us/step - loss: 0.0046\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 19s 311us/step - loss: 0.0037\n"
     ]
    }
   ],
   "source": [
    "# Compilation\n",
    "model.compile(optimizer='adadelta', loss='categorical_crossentropy')\n",
    "\n",
    "# Training\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=5)\n",
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a random sample of 10 test images, their predicted labels and ground truth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 15 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "x_hat = model.predict(x_test)\n",
    "\n",
    "figure = plt.figure(figsize=(20, 8))\n",
    "for i, index in enumerate(np.random.choice(x_test.shape[0], size=15, replace=False)):\n",
    "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
    "    # Display each image\n",
    "    ax.imshow(np.squeeze(x_test[index]), cmap=plt.get_cmap('gray'))\n",
    "    predict_index = np.argmax(x_hat[index])\n",
    "    true_index = np.argmax(y_test[index])\n",
    "    # Set the title for each image\n",
    "    ax.set_title(\"{} ({})\".format(true_index, predict_index),\n",
    "                                  color=(\"green\" if predict_index == true_index else \"red\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex. Visualize the weights of the first 15 filters of the first layer, and save them in a list called 'filters':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 15 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize weights\n",
    "figure = plt.figure(figsize=(20, 8))\n",
    "filters = []\n",
    "for i, index in enumerate(range(15)):\n",
    "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
    "    top_layer = model.layers[0]\n",
    "    plt.imshow(top_layer.get_weights()[0][:, :, :, i].squeeze(), cmap='gray')\n",
    "    filters.append(top_layer.get_weights()[0][:, :, :, i].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex: Apply the filters to a test image using convolve2d function (defined at the beginning of the notebook). What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each filter has learned to activate optimally for different features of the image. In particular they usually act as edge detectors and color filters. We will continue this discussion later on with the VGG16 architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex. Break CAPCTHA system with the help of CNNs (see corresponding notebook).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "On the other hand, CIFAR-10 dataset contains 60,000 color images of 32 x 32 pixels in 3 channels divided\n",
    "into 10 classes. Each class contains 6,000 images. The training set contains 50,000 images,\n",
    "while the test sets provides 10,000 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CIFAR data set using `keras.datasets.cifar10.load_data()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "\n",
    "#load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# plot 4 images as gray scale\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a one-hot encoding and normalize the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#float and normalization (X_train, X_test)\n",
    "#Convert to categorical (Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the model according to the following rules:\n",
    "1. Conv layer with 32 filters, each of which with a 3 x 3 size. \n",
    "2. The output dimension is the same one of the input shape, so it will be 32 x 32 and activation is ReLU,\n",
    "3.  After that we have a max-pooling operation with pool size 2 x 2 and a dropout at 25%\n",
    "4. Add a dense network with 512 units and ReLU activation followed by a dropout at 50% and by a softmax layer with 10 classes as output, one for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "#Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Train and run the model. What is the accuracy level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex. Visualize the feature maps learned by the conv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def layer_to_visualize(layer):\n",
    "    inputs = [K.learning_phase()] + model.inputs\n",
    "\n",
    "    _convout1_f = K.function(inputs, [layer.output])\n",
    "    def convout1_f(X):\n",
    "        # The [0] is to disable the training phase flag\n",
    "        return _convout1_f([0] + [X])\n",
    "\n",
    "    convolutions = convout1_f(img_to_visualize)\n",
    "    convolutions = np.squeeze(convolutions)\n",
    "\n",
    "    print ('Shape of conv:', convolutions.shape)\n",
    "    \n",
    "    n = convolutions.shape[0]\n",
    "    n = int(np.ceil(np.sqrt(n)))\n",
    "    \n",
    "    # Visualization of each filter of the layer\n",
    "    fig = plt.figure(figsize=(20,8))\n",
    "    for i in range(len(convolutions)):\n",
    "        ax = fig.add_subplot(n,n,i+1)\n",
    "        ax.imshow(convolutions[i], cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2\n",
    "plt.imshow(X_train[index], cmap=plt.get_cmap('gray'))\n",
    "plt.show()\n",
    "img_to_visualize = X_train[index]\n",
    "img_to_visualize = np.expand_dims(img_to_visualize, axis=0)\n",
    "\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "layer_to_visualize(layer_dict['conv2d_3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going Deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will discuss two classical, commonly used architectures for convolutional networks: VGG16 and LENET5. As you'll see, these deep CNN architectures apply successively convolutional layers to the input, periodically downsampling the spatial dimensions while increasing the number of feature maps. In essence, we can view these deep arquitectures as rich feature extractors and as a schemes that allows for more efficient learning.\n",
    "\n",
    "Find more detail about deep CNN architectures in the following links:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep CNNs Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [VGG](https://arxiv.org/pdf/1409.1556.pdf)\n",
    "2. [LENET5](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)\n",
    "3. [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n",
    "4. [GoogleNet/Inceptionv1](https://arxiv.org/pdf/1409.4842.pdf)\n",
    "5. [Inceptionv3](https://arxiv.org/pdf/1512.00567.pdf)\n",
    "6. [ResNet](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case: Recognizing cats with a VGG-16 net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG-16 consists of 16 convolutional layers and has a total of about 138 million parameters. All conv layers are 3x3 filters with stride 1 and always use the SAME padding. Max polling layers are 2 by 2 with a strid of 2. \n",
    "\n",
    "Have a look at her architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Alt text](../imgs/13_CNN_vgg16.png)\n",
    "![Alt text](../imgs/13_CNN_vgg_conf.png)\n",
    "*VGGNet achieved excellent results on the ILSVRC-2014 (ImageNet competition) getting the top-5 accuracy (92.3 %) on ImageNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex.- Derive the total number of parameters (take D configuration):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex: Implement VGG16 architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "#define a VGG16 network\n",
    "# input \n",
    "input_layer = Input(shape=(224, 224, 3, 1)), dtype='float32')\n",
    "\n",
    "# 3x3\n",
    "c = Conv2D(filters=16, kernel_size=(3,3), padding='same', activation='relu')(c) \n",
    "c = MaxPooling2D(pool_size=(2,2))(c)\n",
    "c = Conv2D(16, (3,3), padding='same', activation='relu')(c) \n",
    "c = MaxPooling2D(pool_size=(2,2))(c)\n",
    "c = Conv2D(16, (3,3), padding='same', activation='relu')(c) \n",
    "c = MaxPooling2D(pool_size=(2,2))(c)\n",
    "c = Dense(16, activation='relu')(c) \n",
    "x0_conc = Dense(10, activation='softmax')(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your implementation with the one provided by keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import vgg16\n",
    "model = vgg16.VGG16(weights='imagenet', include_top=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the pre-trained VGG-16 model include in keras to train and predict images. Use SGD optimizer and categorical crossentropy loss.\n",
    "Load 'imgs/13_CNN_egyptian_cat.jpeg' using cv2.imread and resize to the dimension (224, 224) with cv2.resize. Is the predicted label equal to 285 (=egyptian cat)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "#resize into VGG16 trained images' format\n",
    "im = cv2.resize(cv2.imread('../imgs/13_CNN_egyptian_cat.jpeg'), (224, 224))\n",
    "im = np.swapaxes(np.swapaxes(im, 1, 2), 0, 1)\n",
    "im = np.expand_dims(im, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the im class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex: Visualize the filters of the block1_conv1 - block5_conv1 layers via gradient ascent in input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import save_img\n",
    "import time\n",
    "\n",
    "# dimensions of the generated pictures for each filter.\n",
    "img_width = 128\n",
    "img_height = 128\n",
    "\n",
    "# the name of the layer we want to visualize\n",
    "# (see model definition at keras/applications/vgg16.py)\n",
    "#layer_name = 'block5_conv1'\n",
    "\n",
    "# util function to convert a tensor into a valid image\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + K.epsilon())\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "\n",
    "# this is the placeholder for the input images\n",
    "input_img = model.input\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for layer_name in ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']:\n",
    "\n",
    "    kept_filters = []\n",
    "    for filter_index in range(64):\n",
    "        # we only scan through the first 200 filters,\n",
    "        # but there are actually 512 of them\n",
    "        #print('Processing filter %d' % filter_index)\n",
    "        start_time = time.time()\n",
    "\n",
    "        # we build a loss function that maximizes the activation\n",
    "        # of the nth filter of the layer considered\n",
    "        layer_output = layer_dict[layer_name].output\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            loss = K.mean(layer_output[:, filter_index, :, :])\n",
    "        else:\n",
    "            loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "        # we compute the gradient of the input picture wrt this loss\n",
    "        grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "        # normalization trick: we normalize the gradient\n",
    "        grads = normalize(grads)\n",
    "\n",
    "        # this function returns the loss and grads given the input picture\n",
    "        iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "        # step size for gradient ascent\n",
    "        step = 1.\n",
    "\n",
    "        # we start from a gray image with some random noise\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            input_img_data = np.random.random((1, 3, img_width, img_height))\n",
    "        else:\n",
    "            input_img_data = np.random.random((1, img_width, img_height, 3))\n",
    "        input_img_data = (input_img_data - 0.5) * 20 + 128\n",
    "\n",
    "        # we run gradient ascent for 20 steps\n",
    "        for i in range(20):\n",
    "            loss_value, grads_value = iterate([input_img_data])\n",
    "            input_img_data += grads_value * step\n",
    "\n",
    "            #print('Current loss value:', loss_value)\n",
    "            if loss_value <= 0.:\n",
    "                # some filters get stuck to 0, we can skip them\n",
    "                break\n",
    "\n",
    "        # decode the resulting input image\n",
    "        if loss_value > 0:\n",
    "            img = deprocess_image(input_img_data[0])\n",
    "            kept_filters.append((img, loss_value))\n",
    "        end_time = time.time()\n",
    "        #print('Filter %d processed in %ds' % (filter_index, end_time - start_time))\n",
    "\n",
    "    # we will stich the best 64 filters on a 8 x 8 grid.\n",
    "    n = 5\n",
    "\n",
    "    # the filters that have the highest loss are assumed to be better-looking.\n",
    "    # we will only keep the top 64 filters.\n",
    "    kept_filters.sort(key=lambda x: x[1], reverse=True)\n",
    "    kept_filters = kept_filters[:n * n]\n",
    "\n",
    "    figure = plt.figure(figsize=(20, 8))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            img, loss = kept_filters[i * n + j]\n",
    "            ax = figure.add_subplot(n, n, i * n + j + 1, xticks=[], yticks=[])\n",
    "            plt.imshow(img)\n",
    "\n",
    "    print(layer_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last figures displayed the patterns each filter maximally responds to or is very active.\n",
    "Note that:\n",
    "\n",
    "- The first layer filters (block1_conv2 and block2_conv2) mostly detect colors, edges and simple shapes.\n",
    "\n",
    "- As we go deeper into the network, the filters build on top of each other, and learn to encode more complex patterns. Can you recognize any of them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smile Dataset & LENET5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will implement the LeNet-5 architecture, a pioneering 7-level convolutional network developed in 1998 by LeCun et al to identify handwritten digits for zip code recognition in the postal service. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Alt text](../imgs/13_CNN_LENET.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lenet-5 consists of 3 convolutional layers (C1, C3 and C5), 2 sub-sampling (pooling) layers (S2 and S4), and 1 fully connected layer (F6), that are followed by the output layer. Convolutional layers use 5 by 5 convolutions with stride 1. Sub-sampling layers are 2 by 2 average pooling layers. Tanh sigmoid activations are used throughout the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement LeNet-5 architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our implementation using the **Smiles dataset**.\n",
    "\n",
    "The SMILES dataset consists of images of faces that are either smiling or not smiling. In total, there are 13,165 grayscale images in the dataset, with each image having a size of 64 × 64 pixels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data from https://github.com/hromi/SMILEsmileD/archive/master.zip and run the next cell to see the number of positive and negative examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "smile_path =  '../data/SMILEsmileD-master/'\n",
    "\n",
    "def list_all_files(directory, extensions=None):\n",
    "    for root, dirnames, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            base, ext = os.path.splitext(filename)\n",
    "            joined = os.path.join(root, filename)\n",
    "            if extensions is None or ext.lower() in extensions:\n",
    "                yield joined\n",
    "\n",
    "negative_paths = list(list_all_files(os.path.join(smile_path,'SMILEs/negatives/negatives7/'), ['.jpg']))\n",
    "print('loaded', len(negative_paths), 'negative examples')\n",
    "positive_paths = list(list_all_files(os.path.join(smile_path, 'SMILEs/positives/positives7/'), ['.jpg']))\n",
    "print('loaded', len(positive_paths), 'positive examples')\n",
    "examples = [(path, 0) for path in negative_paths] + [(path, 1) for path in positive_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show some negative and positive images with the help of the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import block_reduce\n",
    "from skimage.io import imread\n",
    "from io import BytesIO\n",
    "import PIL.Image\n",
    "import IPython.display\n",
    "\n",
    "def show_array(a, fmt='png', filename=None):\n",
    "    a = np.uint8(np.clip(a, 0, 255))\n",
    "    image_data = BytesIO()\n",
    "    PIL.Image.fromarray(a).save(image_data, fmt)\n",
    "    if filename is None:\n",
    "        IPython.display.display(IPython.display.Image(data=image_data.getvalue()))\n",
    "    else:\n",
    "        with open(filename, 'w') as f:\n",
    "            image_data.seek(0)\n",
    "            shutil.copyfileobj(image_data, f)\n",
    "\n",
    "def find_rectangle(n, max_ratio=2):\n",
    "    sides = []\n",
    "    square = int(math.sqrt(n))\n",
    "    for w in range(square, max_ratio * square):\n",
    "        h = n / w\n",
    "        used = w * h\n",
    "        leftover = n - used\n",
    "        sides.append((leftover, (w, h)))\n",
    "    return sorted(sides)[0][1]\n",
    "\n",
    "# should work for 1d and 2d images, assumes images are square but can be overriden\n",
    "def make_mosaic(images, n=None, nx=None, ny=None, w=None, h=None):\n",
    "    if n is None and nx is None and ny is None:\n",
    "        nx, ny = find_rectangle(len(images))\n",
    "    else:\n",
    "        nx = n if nx is None else nx\n",
    "        ny = n if ny is None else ny\n",
    "    images = np.array(images)\n",
    "    if images.ndim == 2:\n",
    "        side = int(np.sqrt(len(images[0])))\n",
    "        h = side if h is None else h\n",
    "        w = side if w is None else w\n",
    "        images = images.reshape(-1, h, w)\n",
    "    else:\n",
    "        h = images.shape[1]\n",
    "        w = images.shape[2]\n",
    "    image_gen = iter(images)\n",
    "    mosaic = np.empty((h*ny, w*nx))\n",
    "    for i in range(ny):\n",
    "        ia = (i)*h\n",
    "        ib = (i+1)*h\n",
    "        for j in range(nx):\n",
    "            ja = j*w\n",
    "            jb = (j+1)*w\n",
    "            mosaic[ia:ib, ja:jb] = next(image_gen)\n",
    "    return mosaic\n",
    "\n",
    "def examples_to_dataset(examples, block_size=2):\n",
    "    X = []\n",
    "    y = []\n",
    "    for path, label in examples:\n",
    "        img = imread(path, as_gray=True)\n",
    "        img = block_reduce(img, block_size=(block_size, block_size), func=np.mean)\n",
    "        img = img.reshape((32*32))\n",
    "        X.append(img)\n",
    "        if(label==0):\n",
    "            y.append((1,0))\n",
    "        else:\n",
    "            y.append((0,1))\n",
    "    return np.asarray(X), np.asarray(y)\n",
    "\n",
    "X, Y = examples_to_dataset(examples)\n",
    "X = np.asarray(X,dtype=np.float32)/ 255.\n",
    "Y = np.asarray(Y,dtype=np.int32)\n",
    "print(X.dtype, X.min(), X.max(), X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show Negative images\n",
    "show_array(255 * make_mosaic(X[:len(negative_paths)], 8), fmt='jpeg') # negative at the beginning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show Positive images\n",
    "show_array(255 * make_mosaic(X[-len(positive_paths):], 8), fmt='jpeg') # positive at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct data and labels lists looping over the input images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from imutils import paths\n",
    "import argparse\n",
    "import imutils\n",
    "\n",
    "# loop over the input images\n",
    "data = []\n",
    "labels = []\n",
    "for imagePath in sorted(list(paths.list_images(smile_path))):\n",
    "  \n",
    "    # load the image, pre-process it, and store it in the data list\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = imutils.resize(image, width=28)\n",
    "    image = img_to_array(image)\n",
    "    data.append(image)\n",
    "    # extract the class label from the image path and update the\n",
    "    # labels list\n",
    "    label = imagePath.split(os.path.sep)[-3]\n",
    "    label = \"smiling\" if label == \"positives\" else \"not_smiling\"\n",
    "    labels.append(label)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data and labels are constructed, we can scale the raw pixel intensities to the range [0, 1] and then apply one-hot encoding to the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the raw pixel intensities to the range [0, 1]\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "# convert the labels from integers to vectors\n",
    "le = LabelEncoder().fit(labels)\n",
    "labels = np_utils.to_categorical(le.transform(labels), 2)\n",
    "# account for skew in the labeled data\n",
    "classTotals = labels.sum(axis=0)\n",
    "classWeight = classTotals.max() / classTotals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve computed our class weights, we can move on to partitioning our data into training and testing splits, using 80% of the data for training and 20% for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition the data into training and testing splits using 80% of\n",
    "# the data for training and the remaining 20% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data,\n",
    "labels, test_size=0.20, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to train LeNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Lenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning curve using the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training + testing loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0,15), H.history['loss'], label='train_loss')\n",
    "plt.plot(np.arange(0,15), H.history['val_loss'], label='val_loss')\n",
    "plt.plot(np.arange(0,15), H.history['acc'], label='acc')\n",
    "plt.plot(np.arange(0,15), H.history['val_acc'], label='val_acc')\n",
    "plt.title('Training Loss and Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss/Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 15 epochs we can see that our network is obtaining 93% classification accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex. Fingerprint_Denoising_and_Inpainting.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/DataScienceUB/DeepLearningfromScratch2018\n",
    "\n",
    "Deep Learning for Computer Vision with Python.- Adrian Rosebrock\n",
    "\n",
    "Deep Learning with keras.- Antonio Gulli, Sujit Pal\n",
    "\n",
    "Deep Learning with Python.- Francois Chollet\n",
    "\n",
    "http://colah.github.io/posts/2014-07-Understanding-Convolutions/\n",
    "\n",
    "https://arxiv.org/abs/1603.07285\n",
    "\n",
    "https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html\n",
    "\n",
    "https://github.com/keras-team/keras/blob/master/examples/conv_filter_visualization.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ks_dl_course]",
   "language": "python",
   "name": "conda-env-ks_dl_course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
