{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "celltoolbar": "Tags",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "02-SparkSQL-DataFrames.inclass.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sl9SuKYRlcIo",
        "Xfg_ccqPlcJL",
        "TiZYJdPnlcJu",
        "2wluZqb6lcKP",
        "GqkFOFwPlcUh",
        "xxa8OLp4lcW5",
        "UwoBX0TklcYU"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_AGdeJzpyS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtySme1elcFI",
        "colab_type": "text"
      },
      "source": [
        "# SparkSQL and DataFrames \n",
        "\n",
        "<a href = \"http://yogen.io\"><img src=\"http://yogen.io/assets/logo.svg\" alt=\"yogen\" style=\"width: 200px; float: right;\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSw1Ggrwlmfi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "be2bf730-a177-4481-ecc5-8ff87bb70c43"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.uvigo.es/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
        "!tar -xf spark-2.4.6-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark pyspark==2.4.6\n",
        "import os\n",
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
        "findspark.init()\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 218.4MB 61kB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 44.9MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMFh8XO5lcFQ",
        "colab_type": "text"
      },
      "source": [
        "## RDDs, DataSets, and DataFrames\n",
        "\n",
        "RDDs are the original interface for Spark programming.\n",
        "\n",
        "DataFrames were introduced in 1.3\n",
        "\n",
        "Datasets were introduced in 1.6, and unified with DataFrames in 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCT0DMbMlcFW",
        "colab_type": "text"
      },
      "source": [
        "### Advantages of DataFrames:\n",
        "\n",
        "from https://www.datacamp.com/community/tutorials/apache-spark-python:\n",
        "\n",
        "> More specifically, the performance improvements are due to two things, which you’ll often come across when you’re reading up DataFrames: custom memory management (project Tungsten), which will make sure that your Spark jobs much faster given CPU constraints, and optimized execution plans (Catalyst optimizer), of which the logical plan of the DataFrame is a part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkSfFg1WlcFa",
        "colab_type": "text"
      },
      "source": [
        "## SparkSQL and DataFrames \n",
        "\n",
        "\n",
        "pyspark does not have the Dataset API, which is available only if you use Spark from a statically typed language: Scala or Java.\n",
        "\n",
        "From https://spark.apache.org/docs/2.4.4/sql-programming-guide.html\n",
        "\n",
        "> A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In the Scala API, DataFrame is simply a type alias of Dataset[Row]. While, in Java API, users need to use Dataset&lt;Row> to represent a DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyAR9LMllcFg",
        "colab_type": "text"
      },
      "source": [
        "### The pyspark.sql module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXVfCSIUlcFk",
        "colab_type": "text"
      },
      "source": [
        "Important classes of Spark SQL and DataFrames:\n",
        "\n",
        "* `pyspark.sql.SparkSession` Main entry point for DataFrame and SQL functionality.\n",
        "\n",
        "* `pyspark.sql.DataFrame` A distributed collection of data grouped into named columns.\n",
        "\n",
        "* `pyspark.sql.Column` A column expression in a DataFrame.\n",
        "\n",
        "* `pyspark.sql.Row` A row of data in a DataFrame.\n",
        "\n",
        "* `pyspark.sql.GroupedData` Aggregation methods, returned by DataFrame.groupBy().\n",
        "\n",
        "* `pyspark.sql.DataFrameNaFunctions` Methods for handling missing data (null values).\n",
        "\n",
        "* `pyspark.sql.DataFrameStatFunctions` Methods for statistics functionality.\n",
        "\n",
        "* `pyspark.sql.functions` List of built-in functions available for DataFrame.\n",
        "\n",
        "* `pyspark.sql.types` List of data types available.\n",
        "\n",
        "* `pyspark.sql.Window` For working with window functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7n4KcsglcFx",
        "colab_type": "text"
      },
      "source": [
        "http://spark.apache.org/docs/2.4.4/api/python/pyspark.sql.html\n",
        "\n",
        "https://spark.apache.org/docs/2.4.4/sql-programming-guide.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoN5LCk2lcF1",
        "colab_type": "text"
      },
      "source": [
        "## SparkSession\n",
        "\n",
        "The traditional way to interact with Spark is the SparkContext. In the notebooks we get that from the pyspark driver.\n",
        "\n",
        "From 2.0 we can use SparkSession to replace SparkConf, SparkContext and SQLContext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2dXA1DglcF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2C0HiJTlcGV",
        "colab_type": "text"
      },
      "source": [
        "#### Passing other options to spark session:\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaEZDMzMlcGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark = SparkSession.builder.config('thiscanbeanykey', 'thiscanbeanyvalue').master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxIKB-WklcG5",
        "colab_type": "text"
      },
      "source": [
        "We can check option values in the resulting session like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTBn9FYzlcG_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "5db2ace7-2ac1-4602-d925-825a203de2da"
      },
      "source": [
        "spark.sparkContext.getConf().getAll()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.driver.host', '69700d245be9'),\n",
              " ('spark.rdd.compress', 'True'),\n",
              " ('thiscanbeanykey', 'thiscanbeanyvalue'),\n",
              " ('spark.driver.port', '40691'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.app.id', 'local-1592587182145'),\n",
              " ('spark.master', 'local[*]'),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.ui.showConsoleProgress', 'true'),\n",
              " ('spark.app.name', 'pyspark-shell')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeLyjN1_lcHa",
        "colab_type": "text"
      },
      "source": [
        "### Creating DataFrames\n",
        "\n",
        "SparkSession.createDataFrame: from an RDD, a list or a pandas.DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT1dMmRWlcHh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d40cf107-6caa-4812-8b7a-6fe42094a1f0"
      },
      "source": [
        "help(spark.createDataFrame)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on method createDataFrame in module pyspark.sql.session:\n",
            "\n",
            "createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True) method of pyspark.sql.session.SparkSession instance\n",
            "    Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
            "    \n",
            "    When ``schema`` is a list of column names, the type of each column\n",
            "    will be inferred from ``data``.\n",
            "    \n",
            "    When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
            "    from ``data``, which should be an RDD of either :class:`Row`,\n",
            "    :class:`namedtuple`, or :class:`dict`.\n",
            "    \n",
            "    When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n",
            "    the real data, or an exception will be thrown at runtime. If the given schema is not\n",
            "    :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
            "    :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\".\n",
            "    Each record will also be wrapped into a tuple, which can be converted to row later.\n",
            "    \n",
            "    If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
            "    rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
            "    \n",
            "    :param data: an RDD of any kind of SQL data representation (e.g. row, tuple, int, boolean,\n",
            "        etc.), :class:`list`, or :class:`pandas.DataFrame`.\n",
            "    :param schema: a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
            "        column names, default is ``None``.  The data type string format equals to\n",
            "        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
            "        omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n",
            "        ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`. We can also use\n",
            "        ``int`` as a short name for ``IntegerType``.\n",
            "    :param samplingRatio: the sample ratio of rows used for inferring\n",
            "    :param verifySchema: verify data types of every row against schema.\n",
            "    :return: :class:`DataFrame`\n",
            "    \n",
            "    .. versionchanged:: 2.1\n",
            "       Added verifySchema.\n",
            "    \n",
            "    .. note:: Usage with spark.sql.execution.arrow.enabled=True is experimental.\n",
            "    \n",
            "    >>> l = [('Alice', 1)]\n",
            "    >>> spark.createDataFrame(l).collect()\n",
            "    [Row(_1='Alice', _2=1)]\n",
            "    >>> spark.createDataFrame(l, ['name', 'age']).collect()\n",
            "    [Row(name='Alice', age=1)]\n",
            "    \n",
            "    >>> d = [{'name': 'Alice', 'age': 1}]\n",
            "    >>> spark.createDataFrame(d).collect()\n",
            "    [Row(age=1, name='Alice')]\n",
            "    \n",
            "    >>> rdd = sc.parallelize(l)\n",
            "    >>> spark.createDataFrame(rdd).collect()\n",
            "    [Row(_1='Alice', _2=1)]\n",
            "    >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n",
            "    >>> df.collect()\n",
            "    [Row(name='Alice', age=1)]\n",
            "    \n",
            "    >>> from pyspark.sql import Row\n",
            "    >>> Person = Row('name', 'age')\n",
            "    >>> person = rdd.map(lambda r: Person(*r))\n",
            "    >>> df2 = spark.createDataFrame(person)\n",
            "    >>> df2.collect()\n",
            "    [Row(name='Alice', age=1)]\n",
            "    \n",
            "    >>> from pyspark.sql.types import *\n",
            "    >>> schema = StructType([\n",
            "    ...    StructField(\"name\", StringType(), True),\n",
            "    ...    StructField(\"age\", IntegerType(), True)])\n",
            "    >>> df3 = spark.createDataFrame(rdd, schema)\n",
            "    >>> df3.collect()\n",
            "    [Row(name='Alice', age=1)]\n",
            "    \n",
            "    >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
            "    [Row(name='Alice', age=1)]\n",
            "    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
            "    [Row(0=1, 1=2)]\n",
            "    \n",
            "    >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
            "    [Row(a='Alice', b=1)]\n",
            "    >>> rdd = rdd.map(lambda row: row[1])\n",
            "    >>> spark.createDataFrame(rdd, \"int\").collect()\n",
            "    [Row(value=1)]\n",
            "    >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
            "    Traceback (most recent call last):\n",
            "        ...\n",
            "    Py4JJavaError: ...\n",
            "    \n",
            "    .. versionadded:: 2.0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIKBFQn1o3Pk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "6df24a04-f06a-4b32-d06f-7777f2081a4d"
      },
      "source": [
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "races = random.choices(['elf', 'hobbit', 'troll'], k=15)\n",
        "ids = range(15)\n",
        "races"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hobbit',\n",
              " 'elf',\n",
              " 'elf',\n",
              " 'elf',\n",
              " 'troll',\n",
              " 'troll',\n",
              " 'troll',\n",
              " 'elf',\n",
              " 'hobbit',\n",
              " 'elf',\n",
              " 'elf',\n",
              " 'hobbit',\n",
              " 'elf',\n",
              " 'elf',\n",
              " 'hobbit']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61dAtahjp1_Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "09870a53-637c-40f1-f61f-5a9cb8b3d9be"
      },
      "source": [
        "data = list(zip(ids, races))\n",
        "data"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 'hobbit'),\n",
              " (1, 'elf'),\n",
              " (2, 'elf'),\n",
              " (3, 'elf'),\n",
              " (4, 'troll'),\n",
              " (5, 'troll'),\n",
              " (6, 'troll'),\n",
              " (7, 'elf'),\n",
              " (8, 'hobbit'),\n",
              " (9, 'elf'),\n",
              " (10, 'elf'),\n",
              " (11, 'hobbit'),\n",
              " (12, 'elf'),\n",
              " (13, 'elf'),\n",
              " (14, 'hobbit')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG_YVwQWpd5L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a75b047-be20-419b-eb0b-651a6c0748bc"
      },
      "source": [
        "df = spark.createDataFrame(data)\n",
        "df"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[_1: bigint, _2: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akvnpC95qoF8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "aa7d76b7-011f-4a09-ce86-7bb6e78505de"
      },
      "source": [
        "df.take(5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(_1=0, _2='hobbit'),\n",
              " Row(_1=1, _2='elf'),\n",
              " Row(_1=2, _2='elf'),\n",
              " Row(_1=3, _2='elf'),\n",
              " Row(_1=4, _2='troll')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_PZJx_TrKtw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49e0256e-9413-46c9-f36b-c12761fc280a"
      },
      "source": [
        "df.rdd"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MapPartitionsRDD[11] at javaToPython at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWmjXmzHramh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e78a9859-0a6b-483e-aefe-4c3e64b57602"
      },
      "source": [
        "df.first()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(_1=0, _2='hobbit')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O79eDLqVrdb_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1bf6667-b535-4c52-8158-cf2cce32dac0"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "Row(hairstyle='bald', height=1.8, name='Bruce Willis')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(hairstyle='bald', height=1.8, name='Bruce Willis')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S6ciHtKruha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "0e452ad9-c385-42a1-b083-0c2f9b832694"
      },
      "source": [
        "data = [ Row(id_=id_, race=race) for id_, race in zip(ids, races) ]\n",
        "data"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id_=0, race='hobbit'),\n",
              " Row(id_=1, race='elf'),\n",
              " Row(id_=2, race='elf'),\n",
              " Row(id_=3, race='elf'),\n",
              " Row(id_=4, race='troll'),\n",
              " Row(id_=5, race='troll'),\n",
              " Row(id_=6, race='troll'),\n",
              " Row(id_=7, race='elf'),\n",
              " Row(id_=8, race='hobbit'),\n",
              " Row(id_=9, race='elf'),\n",
              " Row(id_=10, race='elf'),\n",
              " Row(id_=11, race='hobbit'),\n",
              " Row(id_=12, race='elf'),\n",
              " Row(id_=13, race='elf'),\n",
              " Row(id_=14, race='hobbit')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46NeMRuVsASz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bdcb57da-d3f1-431c-a7b2-912cdd286b45"
      },
      "source": [
        "df = spark.createDataFrame(data)\n",
        "df"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id_: bigint, race: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FypWabvlcIA",
        "colab_type": "text"
      },
      "source": [
        "### Creating DataFrames\n",
        "\n",
        "* From RDDs\n",
        "* from Hive tables\n",
        "* From Spark sources: parquet (default), json, jdbc, orc, libsvm, csv, text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU1DuT7ilcIF",
        "colab_type": "text"
      },
      "source": [
        "#### From RDDs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moGscE4hlcIJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "c34d9c96-2f27-444a-bed5-8262c752c45d"
      },
      "source": [
        "coupon_rdd = spark.sparkContext.textFile('coupon150720.csv').map(lambda line: line.split(','))\n",
        "coupon_rdd.take(2)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['79062005698500',\n",
              "  '1',\n",
              "  'MAA',\n",
              "  'AUH',\n",
              "  '9W',\n",
              "  '9W',\n",
              "  '56.79',\n",
              "  'USD',\n",
              "  '1',\n",
              "  'H',\n",
              "  'H',\n",
              "  '0526',\n",
              "  '150904',\n",
              "  'OK',\n",
              "  'IAF0'],\n",
              " ['79062005698500',\n",
              "  '2',\n",
              "  'AUH',\n",
              "  'CDG',\n",
              "  '9W',\n",
              "  '9W',\n",
              "  '84.34',\n",
              "  'USD',\n",
              "  '1',\n",
              "  'H',\n",
              "  'H',\n",
              "  '6120',\n",
              "  '150905',\n",
              "  'OK',\n",
              "  'IAF0']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43YyNXa4tNs6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f154cd0d-89b3-4bd6-af65-8403581c90ed"
      },
      "source": [
        "spark.createDataFrame(coupon_rdd)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[_1: string, _2: string, _3: string, _4: string, _5: string, _6: string, _7: string, _8: string, _9: string, _10: string, _11: string, _12: string, _13: string, _14: string, _15: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl9SuKYRlcIo",
        "colab_type": "text"
      },
      "source": [
        "### Inferring and specifying schemas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbnfN5bGlcIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xfg_ccqPlcJL",
        "colab_type": "text"
      },
      "source": [
        "#### Fully specifying a schema\n",
        "\n",
        "We need to create a `StructType` composed of `StructField`s. each of those specifies afiled with name, type and `nullable` properties. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXiISBtUlcJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiZYJdPnlcJu",
        "colab_type": "text"
      },
      "source": [
        "#### From csv files\n",
        "\n",
        "We can either read them directly into dataframes or read them as RDDs and transform that into a DataFrame. This second way will be very useful if we have unstructured data like web server logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKh_SFfrlcJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wluZqb6lcKP",
        "colab_type": "text"
      },
      "source": [
        "#### From other types of data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxU-S-ONlcKe",
        "colab_type": "text"
      },
      "source": [
        "Apache Parquet is a free and open-source column-oriented data store of the Apache Hadoop ecosystem. It is similar to the other columnar storage file formats available in Hadoop namely RCFile and Optimized RCFile. It is compatible with most of the data processing frameworks in the Hadoop environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKREmkTllcKl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6p6coYmlcK8",
        "colab_type": "text"
      },
      "source": [
        "### Basic operations with DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu2fLjm6lcLC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "15fae25c-be44-410f-d5bd-33893f08d6c7"
      },
      "source": [
        "x = df.show(5)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+\n",
            "|id_|  race|\n",
            "+---+------+\n",
            "|  0|hobbit|\n",
            "|  1|   elf|\n",
            "|  2|   elf|\n",
            "|  3|   elf|\n",
            "|  4| troll|\n",
            "+---+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wECtZ4zTvFPM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97ca885d-151b-4877-91ab-fa8818a08f5e"
      },
      "source": [
        "print(x)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsY8-uqalcLX",
        "colab_type": "text"
      },
      "source": [
        "### Filtering and selecting\n",
        "\n",
        "Syntax inspired in SQL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDB8wbCvlcLh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "9acc1500-4bb2-46fc-a959-a239f59305d2"
      },
      "source": [
        "df.select('race').show()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+\n",
            "|  race|\n",
            "+------+\n",
            "|hobbit|\n",
            "|   elf|\n",
            "|   elf|\n",
            "|   elf|\n",
            "| troll|\n",
            "| troll|\n",
            "| troll|\n",
            "|   elf|\n",
            "|hobbit|\n",
            "|   elf|\n",
            "|   elf|\n",
            "|hobbit|\n",
            "|   elf|\n",
            "|   elf|\n",
            "|hobbit|\n",
            "+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8BPpuFyvs5O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ca02bc9-5d74-4cdc-8a67-66ab8020a106"
      },
      "source": [
        "df.select('race')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[race: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtKxjg5flcLy",
        "colab_type": "text"
      },
      "source": [
        "If we want to filter, we will need to build an instance of `Column`, using square bracket notation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFJASVFHlcL2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "7b5f0237-e025-4644-e6e9-3ac4c3292078"
      },
      "source": [
        "df['race'].take()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-1454f0234d44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'race'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55QlyoUhwDMH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "2458ea1d-99a1-4113-8f05-664be804828b"
      },
      "source": [
        "df.filter(df['race']=='elf').show()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+\n",
            "|id_|race|\n",
            "+---+----+\n",
            "|  1| elf|\n",
            "|  2| elf|\n",
            "|  3| elf|\n",
            "|  7| elf|\n",
            "|  9| elf|\n",
            "| 10| elf|\n",
            "| 12| elf|\n",
            "| 13| elf|\n",
            "+---+----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-45FeJQiwRon",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "12ebbb1c-4aaa-4a28-9407-c04b600bb60c"
      },
      "source": [
        "df.filter(df['id_'] < 5).show()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+\n",
            "|id_|  race|\n",
            "+---+------+\n",
            "|  0|hobbit|\n",
            "|  1|   elf|\n",
            "|  2|   elf|\n",
            "|  3|   elf|\n",
            "|  4| troll|\n",
            "+---+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkI4sc2fwp4D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "8b0b741d-ee2c-40bf-9dd2-00cc23717fbd"
      },
      "source": [
        "df.filter('id_' < 5).show()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-b3a8714b8b7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id_'\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdnVb3LzlcMP",
        "colab_type": "text"
      },
      "source": [
        "That's because a comparison between str and int will error out, so spark will not even get the chance to infer to which column we are referring."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "raises-exception"
        ],
        "id": "ZOnEWWTSlcMW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "191d1ef9-33e8-4f09-db76-6a16dcaa37ff"
      },
      "source": [
        "'id_' < 5"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-d6153786330a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m'id_'\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJSYCOKDlcM5",
        "colab_type": "text"
      },
      "source": [
        "`where` is exactly synonimous with `filter`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL6y6fhXlcM-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "e79d5803-c632-475e-bb3a-c2dc6c46ebb1"
      },
      "source": [
        "df.where(df['id_'] < 5).show()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+\n",
            "|id_|  race|\n",
            "+---+------+\n",
            "|  0|hobbit|\n",
            "|  1|   elf|\n",
            "|  2|   elf|\n",
            "|  3|   elf|\n",
            "|  4| troll|\n",
            "+---+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsjjZjGhlcNW",
        "colab_type": "text"
      },
      "source": [
        "A column is quite different to a Pandas Series. It is just a reference to a column, and can only be used to construct sparkSQL expressions (select, where...). It can't be collected or taken as a one-dimensional sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "raises-exception"
        ],
        "id": "nEx6LrUXlcNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe6z37aMlcN0",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "Extract all mythical being ids which correspond to hobbits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OqyMftylcN5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "4b53a921-f5ce-4441-9cee-7e4acddade6b"
      },
      "source": [
        "df.filter(df['race'] == 'hobbit').select('id_').show()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+\n",
            "|id_|\n",
            "+---+\n",
            "|  0|\n",
            "|  8|\n",
            "| 11|\n",
            "| 14|\n",
            "+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHu0Fht4lcON",
        "colab_type": "text"
      },
      "source": [
        "### Adding columns\n",
        "\n",
        "Dataframes are immutable, since they are built on top of RDDs, so we can not assign to them. We need to create new DataFrames with the appropriate columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "raises-exception"
        ],
        "id": "01PKNnCulcOQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "76532461-7b64-4509-e647-d7e001b8b824"
      },
      "source": [
        "df[0] = 'elf'"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-13bc16f510e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'elf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object does not support item assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_19WV8Bz3rY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0317e084-81a6-421b-8d01-307a7d412c2c"
      },
      "source": [
        "df['id_'] ** 2"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Column<b'POWER(id_, 2)'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrKrqAu3zfAF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "4c1af6f2-9e58-4391-fa71-6517376d00a1"
      },
      "source": [
        "df2 = df.withColumn('newcol', df['id_'] ** 2)\n",
        "df2.show()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+------+\n",
            "|id_|  race|newcol|\n",
            "+---+------+------+\n",
            "|  0|hobbit|   0.0|\n",
            "|  1|   elf|   1.0|\n",
            "|  2|   elf|   4.0|\n",
            "|  3|   elf|   9.0|\n",
            "|  4| troll|  16.0|\n",
            "|  5| troll|  25.0|\n",
            "|  6| troll|  36.0|\n",
            "|  7|   elf|  49.0|\n",
            "|  8|hobbit|  64.0|\n",
            "|  9|   elf|  81.0|\n",
            "| 10|   elf| 100.0|\n",
            "| 11|hobbit| 121.0|\n",
            "| 12|   elf| 144.0|\n",
            "| 13|   elf| 169.0|\n",
            "| 14|hobbit| 196.0|\n",
            "+---+------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03Ybiwiv0P3S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fefcfde6-faff-4654-c71e-9e64b19c5157"
      },
      "source": [
        "df.select('id_', df['id_'] * 100)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id_: bigint, (id_ * 100): bigint]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT8rKHGflcOp",
        "colab_type": "text"
      },
      "source": [
        "### User defined functions\n",
        "\n",
        "There are many useful functions in pyspark.sql.functions. These work on columns, that is, they are vectorial.\n",
        "\n",
        "We can write User Defined Functions (`udf`s), which allow us to \"vectorize\" operations: write a standard function to process single elements, then build a udf with that that works on columns in a DataFrame, like a SQL function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIFR7-7QlcOu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e036f8d-3e85-4bf4-894f-6b28daff69b2"
      },
      "source": [
        "from pyspark.sql import functions as f\n",
        "\n",
        "df.select('id_',\n",
        "          'race',\n",
        "          f.tanh('id_'),\n",
        "          f.log1p('id_'))\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id_: bigint, race: string, TANH(id_): double, LOG1P(id_): double]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-QGMnK53LZp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "62dd93a1-72d7-4d90-e59e-a85a48174322"
      },
      "source": [
        "import math\n",
        "\n",
        "math.factorial(5)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4q-LRPQ3hRu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "bb42fed1-c7f9-413b-ec38-5839ddc78ba7"
      },
      "source": [
        "math.factorial('id_')"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-c638a04ebe8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactorial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: an integer is required (got type str)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRbDn_3t3mSX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "5b5b3c0f-5260-4df0-b7d1-0e9b573b89e5"
      },
      "source": [
        "math.factorial(df['id_'])"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-f365555121eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactorial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id_'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: an integer is required (got type Column)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be1QjJMQlcPA",
        "colab_type": "text"
      },
      "source": [
        "This errors out because \n",
        "\n",
        "```python\n",
        "math.factorial\n",
        "```\n",
        "\n",
        "is not a udf: it doesn't know how to work with strings or Column objects:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF6X4Kf5lcPR",
        "colab_type": "text"
      },
      "source": [
        "But we can transform it into a udf:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJXZ3EpIlcPY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c54a23e-2bea-44f7-b892-96e4b78a1e25"
      },
      "source": [
        "factorial_udf = f.udf(math.factorial) \n",
        "factorial_udf"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function math.factorial>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH9f-Yae4JBF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "812c2820-24b9-422f-c48d-6afc1d9505f1"
      },
      "source": [
        "factorial_udf('id_')"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Column<b'factorial(id_)'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqGLmdgblcPx",
        "colab_type": "text"
      },
      "source": [
        "We can do the same with any function we dream up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY5p8mc9lcP1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "a7aaee23-9013-45d0-9d9c-9b844a37ab12"
      },
      "source": [
        "panoli_malote = f.udf(lambda: random.choice(['panoli', 'malote']))\n",
        "dice_12 = f.udf(lambda: random.randrange(1,13))\n",
        "baddies = df.select('*',\n",
        "                    panoli_malote(),\n",
        "                    dice_12())\n",
        "baddies.show()"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+----------+----------+\n",
            "|id_|  race|<lambda>()|<lambda>()|\n",
            "+---+------+----------+----------+\n",
            "|  0|hobbit|    malote|         6|\n",
            "|  1|   elf|    panoli|         5|\n",
            "|  2|   elf|    panoli|         9|\n",
            "|  3|   elf|    panoli|         7|\n",
            "|  4| troll|    panoli|         4|\n",
            "|  5| troll|    panoli|         8|\n",
            "|  6| troll|    panoli|         6|\n",
            "|  7|   elf|    panoli|         3|\n",
            "|  8|hobbit|    malote|         4|\n",
            "|  9|   elf|    malote|         9|\n",
            "| 10|   elf|    panoli|        10|\n",
            "| 11|hobbit|    malote|         7|\n",
            "| 12|   elf|    panoli|         5|\n",
            "| 13|   elf|    panoli|         8|\n",
            "| 14|hobbit|    panoli|         3|\n",
            "+---+------+----------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECWJq0GulcQL",
        "colab_type": "text"
      },
      "source": [
        "If we want the resulting columns to be of a particular type, we need to specify the return type. This is because in Python return types can not be inferred."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5OYQVp5lcQQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d1ca1a4-f8da-4a88-e37c-f1764ed6d3cf"
      },
      "source": [
        "from pyspark.sql import types\n",
        "\n",
        "panoli_malote = f.udf(lambda: random.choice(['panoli', 'malote']))\n",
        "dice_12 = f.udf(lambda: random.randrange(1,13), returnType=types.IntegerType())\n",
        "baddies = df.select('*',\n",
        "                    panoli_malote(),\n",
        "                    dice_12())\n",
        "baddies"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id_: bigint, race: string, <lambda>(): string, <lambda>(): int]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJYPgDHalcQn",
        "colab_type": "text"
      },
      "source": [
        "Think about this function: what is its return type?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "girUfC_AlcQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def something(a, b):\n",
        "  return a + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TABVvxmj6Z0x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "db9a806e-ad3d-4a2f-9520-b0b1d86d7836"
      },
      "source": [
        "from pyspark.sql import types\n",
        "\n",
        "panoli_malote = f.udf(lambda: random.choice(['panoli', 'malote']))\n",
        "dice_12 = f.udf(lambda: random.randrange(1,13))\n",
        "baddies = df.select('*',\n",
        "                    panoli_malote().alias('alignment'),\n",
        "                    dice_12().cast(types.IntegerType()).alias('dexterity'),\n",
        "                    dice_12().cast(types.IntegerType()).alias('strength'))\n",
        "baddies.show()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+---------+---------+--------+\n",
            "|id_|  race|alignment|dexterity|strength|\n",
            "+---+------+---------+---------+--------+\n",
            "|  0|hobbit|   malote|       11|       9|\n",
            "|  1|   elf|   panoli|        1|       8|\n",
            "|  2|   elf|   panoli|        7|      10|\n",
            "|  3|   elf|   malote|       12|      12|\n",
            "|  4| troll|   panoli|        3|       9|\n",
            "|  5| troll|   malote|        4|       9|\n",
            "|  6| troll|   malote|        4|       4|\n",
            "|  7|   elf|   panoli|       12|       7|\n",
            "|  8|hobbit|   panoli|        7|       7|\n",
            "|  9|   elf|   panoli|        4|      11|\n",
            "| 10|   elf|   panoli|       12|       7|\n",
            "| 11|hobbit|   panoli|       11|       5|\n",
            "| 12|   elf|   malote|        1|       9|\n",
            "| 13|   elf|   panoli|        2|      12|\n",
            "| 14|hobbit|   panoli|        8|       1|\n",
            "+---+------+---------+---------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrXq0thGlcRN",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise: \n",
        "\n",
        "Create an 'hp' field in our df. make it 30000 for hobbits, 40000 for elves and 70000 for trolls.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvRFwzv3lcRT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "455a0881-6537-4b48-d3ca-4e8e4e49e7f1"
      },
      "source": [
        "race = 'hobbit'\n",
        "\n",
        "def hp_from_race(race):\n",
        "\n",
        "  if race == 'hobbit': \n",
        "    return 30000\n",
        "  elif race == 'elf':\n",
        "    return 40000\n",
        "  elif race == 'troll':\n",
        "    return 70000\n",
        "\n",
        "hp_from_race(race)\n",
        "\n",
        "hp_udf = f.udf(hp_from_race, returnType=types.IntegerType())\n",
        "\n",
        "with_hp = baddies.withColumn('hp', hp_udf('race'))\n",
        "with_hp"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id_: bigint, race: string, alignment: string, dexterity: int, strength: int, hp: int]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znmfI3IYlcR9",
        "colab_type": "text"
      },
      "source": [
        "If we have a column that is not the desired type, we can convert it with `cast`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMkzl9WflcSa",
        "colab_type": "text"
      },
      "source": [
        "### Summary statistics\n",
        "\n",
        "https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBYcdDIUlcSf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf48747f-257d-4170-d655-be6b682a3122"
      },
      "source": [
        "with_hp.corr('hp', 'dexterity')"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.16485986761022522"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSA_UgBTAStQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ba34e28-9d24-4c87-95c1-d26c52da31a4"
      },
      "source": [
        "with_hp.stat.cov('dexterity', 'strength')"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.5714285714285713"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYBcEdXulcS4",
        "colab_type": "text"
      },
      "source": [
        "### .crosstab()\n",
        "\n",
        "Crosstab returns the contingency table for two columns, as a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gInTiw-YlcS8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "c2f3bbe1-c64f-4089-8702-c089617139da"
      },
      "source": [
        "with_hp.cache().crosstab('alignment', 'race').show()"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------+---+------+-----+\n",
            "|alignment_race|elf|hobbit|troll|\n",
            "+--------------+---+------+-----+\n",
            "|        malote|  5|     1|    2|\n",
            "|        panoli|  3|     3|    1|\n",
            "+--------------+---+------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBP4fEe4lcTK",
        "colab_type": "text"
      },
      "source": [
        "### Grouping\n",
        "\n",
        "Grouping works very similarly to Pandas: executing groupby (or groupBy) on a DataFrame will return an object (a GroupedData) that can then be aggregated to obtain the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtZdgRvClcTN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5791c980-fbd9-4af0-c225-d45dc3197c04"
      },
      "source": [
        "gd = with_hp.groupby('race')\n",
        "gd"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.group.GroupedData at 0x7f542d7110f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSbMDz02lcTf",
        "colab_type": "text"
      },
      "source": [
        "GroupedData has several aggregation functions defined:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHIAQP4HlcTj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "4ac93750-92f3-42a3-c85e-4659777b1f9e"
      },
      "source": [
        "gd.mean('strength').show()"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+-----------------+\n",
            "|  race|    avg(strength)|\n",
            "+------+-----------------+\n",
            "| troll|6.666666666666667|\n",
            "|hobbit|             7.25|\n",
            "|   elf|             8.75|\n",
            "+------+-----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UAvCq4nlcTx",
        "colab_type": "text"
      },
      "source": [
        "We can do several aggregations in a single step, with a number of different syntaxes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ-Y7VxxlcT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "8494e152-10fd-44dd-df50-1eb875b15410"
      },
      "source": [
        "gd.agg({'strength' : 'avg', 'dexterity': 'stddev'}).show()"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+------------------+-----------------+\n",
            "|  race| stddev(dexterity)|    avg(strength)|\n",
            "+------+------------------+-----------------+\n",
            "| troll| 4.618802153517007|6.666666666666667|\n",
            "|hobbit|1.2909944487358056|             7.25|\n",
            "|   elf| 3.845219666769935|             8.75|\n",
            "+------+------------------+-----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTZ70ck0FiaQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "110b2266-4cff-46ac-dd1f-132bc4f9be66"
      },
      "source": [
        "gd.agg(f.mean('dexterity'), \n",
        "       f.stddev('dexterity'),\n",
        "       f.max('dexterity'),\n",
        "       f.min('dexterity')).show()"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+-----------------+----------------------+--------------+--------------+\n",
            "|  race|   avg(dexterity)|stddev_samp(dexterity)|max(dexterity)|min(dexterity)|\n",
            "+------+-----------------+----------------------+--------------+--------------+\n",
            "| troll|6.666666666666667|     4.618802153517007|            12|             4|\n",
            "|hobbit|              8.5|    1.2909944487358056|            10|             7|\n",
            "|   elf|             6.75|     3.845219666769935|            12|             1|\n",
            "+------+-----------------+----------------------+--------------+--------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw4blXQblcUD",
        "colab_type": "text"
      },
      "source": [
        "### Intersections\n",
        "\n",
        "Ver much like SQL joins. We can specify the columns and the join method (left, right, inner, outer) or we can let Spark infer them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgzIn0OFlcUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stats = gd.agg(f.mean('dexterity'), \n",
        "               f.stddev('dexterity'),\n",
        "               f.max('dexterity'),\n",
        "               f.min('dexterity'))\n"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yig4mLlQGcxX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e2fed713-26f0-4694-b37b-dcb8e7cf8390"
      },
      "source": [
        "with_hp.join(stats).show()"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1211.showString.\n: org.apache.spark.sql.AnalysisException: Detected implicit cartesian product for INNER join between logical plans\nInMemoryRelation [id_#8L, race#9, alignment#285, dexterity#287, strength#289, hp#344], StorageLevel(disk, memory, deserialized, 1 replicas)\n   +- *(1) Project [id_#8L, race#9, pythonUDF0#1258 AS alignment#285, cast(pythonUDF1#1259 as int) AS dexterity#287, cast(pythonUDF2#1260 as int) AS strength#289, pythonUDF3#1261 AS hp#344]\n      +- BatchEvalPython [<lambda>(), <lambda>(), <lambda>(), hp_from_race(race#9)], [id_#8L, race#9, pythonUDF0#1258, pythonUDF1#1259, pythonUDF2#1260, pythonUDF3#1261]\n         +- Scan ExistingRDD[id_#8L,race#9]\nand\nAggregate [race#2071], [race#2071, avg(cast(dexterity#287 as bigint)) AS avg(dexterity)#2016, stddev_samp(cast(dexterity#287 as double)) AS stddev_samp(dexterity)#2025, max(dexterity#287) AS max(dexterity)#2026, min(dexterity#287) AS min(dexterity)#2027]\n+- Project [race#2071, dexterity#287]\n   +- InMemoryRelation [id_#2070L, race#2071, alignment#285, dexterity#287, strength#289, hp#344], StorageLevel(disk, memory, deserialized, 1 replicas)\n         +- *(1) Project [id_#8L, race#9, pythonUDF0#1258 AS alignment#285, cast(pythonUDF1#1259 as int) AS dexterity#287, cast(pythonUDF2#1260 as int) AS strength#289, pythonUDF3#1261 AS hp#344]\n            +- BatchEvalPython [<lambda>(), <lambda>(), <lambda>(), hp_from_race(race#9)], [id_#8L, race#9, pythonUDF0#1258, pythonUDF1#1259, pythonUDF2#1260, pythonUDF3#1261]\n               +- Scan ExistingRDD[id_#8L,race#9]\nJoin condition is missing or trivial.\nEither: use the CROSS JOIN syntax to allow cartesian products between these\nrelations, or: enable implicit cartesian products by setting the configuration\nvariable spark.sql.crossJoin.enabled=true;\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$22.applyOrElse(Optimizer.scala:1295)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$22.applyOrElse(Optimizer.scala:1292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:258)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:258)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:257)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:247)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1292)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1274)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:67)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:67)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:69)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:78)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3365)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor101.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-148-fd7c0e82dbca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwith_hp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: 'Detected implicit cartesian product for INNER join between logical plans\\nInMemoryRelation [id_#8L, race#9, alignment#285, dexterity#287, strength#289, hp#344], StorageLevel(disk, memory, deserialized, 1 replicas)\\n   +- *(1) Project [id_#8L, race#9, pythonUDF0#1258 AS alignment#285, cast(pythonUDF1#1259 as int) AS dexterity#287, cast(pythonUDF2#1260 as int) AS strength#289, pythonUDF3#1261 AS hp#344]\\n      +- BatchEvalPython [<lambda>(), <lambda>(), <lambda>(), hp_from_race(race#9)], [id_#8L, race#9, pythonUDF0#1258, pythonUDF1#1259, pythonUDF2#1260, pythonUDF3#1261]\\n         +- Scan ExistingRDD[id_#8L,race#9]\\nand\\nAggregate [race#2071], [race#2071, avg(cast(dexterity#287 as bigint)) AS avg(dexterity)#2016, stddev_samp(cast(dexterity#287 as double)) AS stddev_samp(dexterity)#2025, max(dexterity#287) AS max(dexterity)#2026, min(dexterity#287) AS min(dexterity)#2027]\\n+- Project [race#2071, dexterity#287]\\n   +- InMemoryRelation [id_#2070L, race#2071, alignment#285, dexterity#287, strength#289, hp#344], StorageLevel(disk, memory, deserialized, 1 replicas)\\n         +- *(1) Project [id_#8L, race#9, pythonUDF0#1258 AS alignment#285, cast(pythonUDF1#1259 as int) AS dexterity#287, cast(pythonUDF2#1260 as int) AS strength#289, pythonUDF3#1261 AS hp#344]\\n            +- BatchEvalPython [<lambda>(), <lambda>(), <lambda>(), hp_from_race(race#9)], [id_#8L, race#9, pythonUDF0#1258, pythonUDF1#1259, pythonUDF2#1260, pythonUDF3#1261]\\n               +-..."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9W7If5qlcUS",
        "colab_type": "text"
      },
      "source": [
        "Spark refuses to do cross joins by default. To perform them, we can \n",
        "\n",
        "a) Allow then explicitly:\n",
        "\n",
        "```python\n",
        "session.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")\n",
        "```\n",
        "\n",
        "b) Specify the join criterion\n",
        "\n",
        "```python\n",
        "df4.join(new_df, on='id').show()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "raises-exception"
        ],
        "id": "S5ZAubYnlcUV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "e059e79f-6b41-43d7-bf28-a717053566c7"
      },
      "source": [
        "with_hp.join(stats, on='race').show()"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+---+---------+---------+--------+-----+-----------------+----------------------+--------------+--------------+\n",
            "|  race|id_|alignment|dexterity|strength|   hp|   avg(dexterity)|stddev_samp(dexterity)|max(dexterity)|min(dexterity)|\n",
            "+------+---+---------+---------+--------+-----+-----------------+----------------------+--------------+--------------+\n",
            "|hobbit|  0|   malote|        9|       4|30000|              8.5|    1.2909944487358056|            10|             7|\n",
            "|   elf|  1|   malote|       10|      11|40000|             6.75|     3.845219666769935|            12|             1|\n",
            "|   elf|  2|   malote|       12|      12|40000|             6.75|     3.845219666769935|            12|             1|\n",
            "|   elf|  3|   panoli|        3|       9|40000|             6.75|     3.845219666769935|            12|             1|\n",
            "| troll|  4|   malote|        4|       9|70000|6.666666666666667|     4.618802153517007|            12|             4|\n",
            "| troll|  5|   malote|        4|       4|70000|6.666666666666667|     4.618802153517007|            12|             4|\n",
            "| troll|  6|   panoli|       12|       7|70000|6.666666666666667|     4.618802153517007|            12|             4|\n",
            "|   elf|  7|   malote|        4|       6|40000|             6.75|     3.845219666769935|            12|             1|\n",
            "|hobbit|  8|   panoli|       10|      11|30000|              8.5|    1.2909944487358056|            10|             7|\n",
            "|   elf|  9|   malote|        7|       4|40000|             6.75|     3.845219666769935|            12|             1|\n",
            "|   elf| 10|   malote|       10|       9|40000|             6.75|     3.845219666769935|            12|             1|\n",
            "|hobbit| 11|   panoli|        8|       4|30000|              8.5|    1.2909944487358056|            10|             7|\n",
            "|   elf| 12|   panoli|        7|      11|40000|             6.75|     3.845219666769935|            12|             1|\n",
            "|   elf| 13|   panoli|        1|       8|40000|             6.75|     3.845219666769935|            12|             1|\n",
            "|hobbit| 14|   panoli|        7|      10|30000|              8.5|    1.2909944487358056|            10|             7|\n",
            "+------+---+---------+---------+--------+-----+-----------------+----------------------+--------------+--------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "GqkFOFwPlcUh",
        "colab_type": "text"
      },
      "source": [
        "#### Digression\n",
        "\n",
        "We can monitor our running jobs and storage used at the Spark Web UI. We can get its url with sc.uiWebUrl.\n",
        "\n",
        "StorageLevels represent how our DataFrame is cached: we can save the results of the computation up to that point, so that if we process several times the same data only the subsequent steps will be recomputed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y49DulAlcUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efMW6_bzlcU7",
        "colab_type": "text"
      },
      "source": [
        "We can erase it with `unpersist`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amNYPY93lcVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31-M0PFllcVU",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "Calculate the [z-score](http://www.statisticshowto.com/probability-and-statistics/z-score/) of each creature's hp for their alignment\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx_KIZYXlcVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-xN7Da2lcVu",
        "colab_type": "text"
      },
      "source": [
        "1) Calculate the mean and std of salary for each location"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5-oTjrWlcVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvH4CcZZlcWC",
        "colab_type": "text"
      },
      "source": [
        "2) Annotate each employee with the stats corresponding to their location"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80iVii0ClcWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtuOW6UDlcWW",
        "colab_type": "text"
      },
      "source": [
        "3) Calculate the z-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_sFjlKKlcWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU-Aj8fJlcWk",
        "colab_type": "text"
      },
      "source": [
        "Note that we can build more complex boolean conditions for joining, as well as joining on columns that do not have the same name:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5yh7EU9lcWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxa8OLp4lcW5",
        "colab_type": "text"
      },
      "source": [
        "### Handling null values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7ihBzQAlcW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maBKVhFqlcXL",
        "colab_type": "text"
      },
      "source": [
        "## SQL querying\n",
        "\n",
        "We need to register our DataFrame as a table in the SQL context in order to be able to query against it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrxE0cAQlcXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPTyHgiElcXc",
        "colab_type": "text"
      },
      "source": [
        "Once registered, we can perform queries as complex as we want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjd1ne8olcXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkSEw-t6lcXq",
        "colab_type": "text"
      },
      "source": [
        "## Interoperation with Pandas\n",
        "\n",
        "Easy peasy. We can convert a spark DataFrame into a Pandas one, which will `collect` it, and viceversa, which will distribute it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYidITSMlcXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMEUFtizlcYE",
        "colab_type": "text"
      },
      "source": [
        "## Writing out\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXLy8G_2lcYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwoBX0TklcYU",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "Repeat the exercise from the previous notebook, but this time with DataFrames.\n",
        "\n",
        "Get stats for all tickets with destination MAD from `coupons150720.csv`.\n",
        "\n",
        "You will need to extract ticket amounts with destination MAD, and then calculate:\n",
        "\n",
        "1. Total ticket amounts per origin\n",
        "2. Top 10 airlines by average amount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT095rtelcYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6x_uCW4lcY1",
        "colab_type": "text"
      },
      "source": [
        "1) Extract the fields you need (c0,c1,c2,c3,c4 and c6) into a dataframe with proper names and types\n",
        "\n",
        "Remember, you want to calculate:\n",
        "\n",
        "Total ticket amounts per origin\n",
        "\n",
        "Top 10 airlines by average amount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeILbLfTlcY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqHVClSplcZF",
        "colab_type": "text"
      },
      "source": [
        "2) Total ticket amounts per origin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87DL0d1DlcZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcIdL7j8lcZX",
        "colab_type": "text"
      },
      "source": [
        "3) Top 10 Airlines by average amount\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IOADhFylcZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNz46A_ulcZm",
        "colab_type": "text"
      },
      "source": [
        "## Further Reading\n",
        "\n",
        "https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/apache-spark-python\n",
        "\n",
        "https://spark.apache.org/docs/2.2.0/sql-programming-guide.html\n",
        "\n",
        "https://ogirardot.wordpress.com/2015/05/29/rdds-are-the-new-bytecode-of-apache-spark/\n",
        "\n",
        "https://stackoverflow.com/questions/36822224/what-are-the-pros-and-cons-of-parquet-format-compared-to-other-formats\n",
        "\n",
        "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf"
      ]
    }
  ]
}