{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porting an analysis from local to distributed\n",
    "\n",
    "<a href = \"http://yogen.io\"><img src=\"http://yogen.io/assets/logo.svg\" alt=\"yogen\" style=\"width: 200px; float: right;\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the opportunity to put in practice what we have just learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided exercise\n",
    "\n",
    "Recreate the boxplot we did in the pandas section, in Spark!\n",
    "\n",
    "Since matplotlib boxplot needs all the data and that would be unfeasible with Big Data, we will calculate the quartiles ourselves.\n",
    "\n",
    "Once the analysis is ported, we will be able to run it on the whole historical series! You can find it at https://transtats.bts.gov (On time performance reporting carrier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Workflow\n",
    "\n",
    "The basic idea is the same that we applied in the Amadeus Challenge:\n",
    "\n",
    "* Build prototype with small data: in this section, we will be using `06-intro_to_pandas_practical.ipynb` as our already made prototype\n",
    "\n",
    "* Modify your prototype so that it works with Big Data: In this case, it means porting it to Spark\n",
    "\n",
    "* Test your \"Big Data\" prototype with small data: We will first test it with a sample locally, then upload it to a cluster and test it with Big Data.\n",
    "\n",
    "    * You can run your analyses building your own cluster and storage bucket in Google Cloud Storage. More in notebook #4!\n",
    "\n",
    "* Run your prototype with Big Data.\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the prototype so that it works with Big Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read csv\n",
    "\n",
    "We'll use the `SparkSession.read.csv` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select relevant columns\n",
    "\n",
    "Literally the same syntax as Pandas!\n",
    "\n",
    "```python\n",
    "df = df.select(['FlightDate', 'DayOfWeek', 'Reporting_Airline', 'Tail_Number', 'Flight_Number_Reporting_Airline', 'Origin', \n",
    "                'OriginCityName', 'OriginStateName', 'Dest', 'DestCityName', 'DestStateName',\n",
    "                'DepTime', 'DepDelay', 'AirTime', 'Distance'])\n",
    "\n",
    "df\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract \"Hour\" variable\n",
    "\n",
    "The DepTimes have been inferred to be floats. We need them as ints, representing each o fthe 24 hours in a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the relative distributions\n",
    "\n",
    "In order to be able to handle the data, we need to reduce its dimensionality. Since we want to describe a discrete distribution, we can just count how many values of each level of the 'DepDelay' variable we find for each hour (24 different discrete distributions). We also want the totals in order to do the relative distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we join both and calculate what fraction of the total for each hour each level of DepDelay represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate distributions\n",
    "\n",
    "We have to group on the hour. Each group will be a bunch of delays and the corresponding frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These groups are definitely manageable: the number of levels will be on the order of a few hundreds to a couple thousands. We can combine them into lists straight away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's be easy to use a UDF to merge the two lists and sort them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Careful! If we keep that string return type, it might be problematic later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the quartiles\n",
    "\n",
    "We are finally ready to calculate the quartiles! We will use a UDF.\n",
    "\n",
    "The input to our custom function will be one of the distributions coded like we did: as a list of tuples `(value, relative_frequency)`. The quartiles are defined as the values at which we cross the 0.0, .25, .5, .75 and 1.00 relative frequencies. Since the distributions are ordered, we can just iterate over one while keeping track of what portion of the total distribution we have seen, and annotate where we cross the thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply to the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "\n",
    "We got it! Let's move this over to Pandas for convenient handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are ready to plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your \"Big Data\" prototype with small data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This is the whole process, collected in one place as is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark job\n",
    "\n",
    "In order to run the process in a cluster, we need to transform it into a pyspark job file. \n",
    "\n",
    "We need to tidy up the function definitions, add the relevant imports, and modify the input and output to use command-line arguments.\n",
    "\n",
    "We will put the result in a file called mysparkjob.py:\n",
    "\n",
    "```python\n",
    "from __future__ import print_function\n",
    "from pyspark.sql import types, functions, SparkSession\n",
    "import sys\n",
    "\n",
    "def zipsort(a, b):\n",
    "    return sorted(zip(a, b))\n",
    "\n",
    "def quartiles(histogram):\n",
    "    area = 0\n",
    "    result = []\n",
    "    \n",
    "    for value, percentage in histogram:\n",
    "        if area == 0:\n",
    "            result.append(value)\n",
    "        elif area <= .25 and area + percentage > .25:\n",
    "            result.append(value)\n",
    "        elif area <= .5 and area + percentage > .5:\n",
    "            result.append(value)\n",
    "        elif area <= .75 and area + percentage > .75:\n",
    "            result.append(value)\n",
    "        area += percentage\n",
    "    \n",
    "    result.append(value)\n",
    "    return result\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    file = sys.argv[1]\n",
    "    out = sys.argv[2]\n",
    "    \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    df = spark.read.csv(file, header= True, inferSchema=True)\n",
    "    df = df.select(['FlightDate', 'DayOfWeek', 'Reporting_Airline', 'Tail_Number', 'Flight_Number_Reporting_Airline', 'Origin', \n",
    "                    'OriginCityName', 'OriginStateName', 'Dest', 'DestCityName', 'DestStateName',\n",
    "                    'DepTime', 'DepDelay', 'AirTime', 'Distance'])\n",
    "\n",
    "    df2 = df.withColumn('Hour', (df['DepTime'] / 100).cast(types.IntegerType()))\n",
    "    totals = df2.groupBy('Hour').count()\n",
    "    distributions = df2.groupBy(['Hour', 'DepDelay']).count()\n",
    "    annotated = distributions.join(totals, on='Hour')\n",
    "    frequencies = annotated.withColumn('relative', distributions['count'] / totals['count'])\n",
    "    groups = frequencies.groupBy(totals['Hour'])\\\n",
    "                        .agg(functions.collect_list('DepDelay').alias('delays'),\n",
    "                             functions.collect_list('relative').alias('relatives'))\n",
    "\n",
    "\n",
    "\n",
    "    zipsort_typed = functions.udf(zipsort, types.ArrayType(types.ArrayType(types.FloatType())))\n",
    "    distributions = groups.withColumn('distributions', zipsort_typed('delays', 'relatives'))\n",
    "\n",
    "\n",
    "\n",
    "    quartiles_udf = functions.udf(quartiles, returnType=types.ArrayType(types.FloatType()))\n",
    "\n",
    "    result = distributions.select('Hour',\n",
    "                                  quartiles_udf('distributions').alias('quartiles'))\n",
    "\n",
    "    result.write.json(out)\n",
    "    spark.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running with spark-submit\n",
    "\n",
    "If the following works, we are ready to test it in the cluster!\n",
    "\n",
    "```python\n",
    "unset PYSPARK_DRIVER_PYTHON\n",
    "spark-submit mysparkjob.py On_Time_On_Time_Performance_2015_8.csv out.csv\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
