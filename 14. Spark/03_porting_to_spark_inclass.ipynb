{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "03-porting-to-spark.inclass.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wfjM2RAHL4B",
        "colab_type": "text"
      },
      "source": [
        "# Porting an analysis from local to distributed\n",
        "\n",
        "<a href = \"http://yogen.io\"><img src=\"http://yogen.io/assets/logo.svg\" alt=\"yogen\" style=\"width: 200px; float: right;\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XtV0dICHL4J",
        "colab_type": "text"
      },
      "source": [
        "Now comes the opportunity to put in practice what we have just learned!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dVJFzFKHL4P",
        "colab_type": "text"
      },
      "source": [
        "# Guided exercise\n",
        "\n",
        "Recreate the boxplot we did in the pandas section, in Spark!\n",
        "\n",
        "Since matplotlib boxplot needs all the data and that would be unfeasible with Big Data, we will calculate the quartiles ourselves.\n",
        "\n",
        "Once the analysis is ported, we will be able to run it on the whole historical series! You can find it at https://transtats.bts.gov (On time performance reporting carrier)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQp-Kz_yHL4W",
        "colab_type": "text"
      },
      "source": [
        "##  Workflow\n",
        "\n",
        "The basic idea is the same that we applied in the Amadeus Challenge:\n",
        "\n",
        "* Build prototype with small data: in this section, we will be using `06-intro_to_pandas_practical.ipynb` as our already made prototype\n",
        "\n",
        "* Modify your prototype so that it works with Big Data: In this case, it means porting it to Spark\n",
        "\n",
        "* Test your \"Big Data\" prototype with small data: We will first test it with a sample locally, then upload it to a cluster and test it with Big Data.\n",
        "\n",
        "    * You can run your analyses building your own cluster and storage bucket in Google Cloud Storage. More in notebook #4!\n",
        "\n",
        "* Run your prototype with Big Data.\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iYXb6lQHL4d",
        "colab_type": "text"
      },
      "source": [
        "## Modify the prototype so that it works with Big Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7mf2NX_HL4i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "c9de478b-62de-4996-eb58-7a18ecd1fedb"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.uvigo.es/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
        "!tar -xf spark-2.4.6-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark pyspark==2.4.6\n",
        "import os\n",
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
        "findspark.init()\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 218.4MB 63kB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 44.0MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKiGXAB7HL5H",
        "colab_type": "text"
      },
      "source": [
        "## Read csv\n",
        "\n",
        "We'll use the `SparkSession.read.csv` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S8fLTfzHL5M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "8724f9c8-b5ab-451a-a762-39eaf32c6604"
      },
      "source": [
        "!ls -lh "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 252M\n",
            "-rw-r--r--  1 root root  29M Jun 20 09:57 On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_12.zip\n",
            "drwxr-xr-x  1 root root 4.0K Jun 17 16:18 sample_data\n",
            "drwxr-xr-x 13 1000 1000 4.0K May 30 00:02 spark-2.4.6-bin-hadoop2.7\n",
            "-rw-r--r--  1 root root 223M May 30 00:54 spark-2.4.6-bin-hadoop2.7.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv9EGIdSKAVH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "f34a55d7-ec30-4af9-9893-917d87d07b6b"
      },
      "source": [
        "!unzip -o On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_12.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_12.zip\n",
            "  inflating: On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2018_12.csv  \n",
            "  inflating: readme.html             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvavmmtjK5Wp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4d68d061-d2a1-4e6d-fa6e-fd4c88b13a1b"
      },
      "source": [
        "df = spark.read.csv('On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2018_12.csv', header=True, inferSchema=True)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Year: int, Quarter: int, Month: int, DayofMonth: int, DayOfWeek: int, FlightDate: timestamp, Reporting_Airline: string, DOT_ID_Reporting_Airline: int, IATA_CODE_Reporting_Airline: string, Tail_Number: string, Flight_Number_Reporting_Airline: int, OriginAirportID: int, OriginAirportSeqID: int, OriginCityMarketID: int, Origin: string, OriginCityName: string, OriginState: string, OriginStateFips: int, OriginStateName: string, OriginWac: int, DestAirportID: int, DestAirportSeqID: int, DestCityMarketID: int, Dest: string, DestCityName: string, DestState: string, DestStateFips: int, DestStateName: string, DestWac: int, CRSDepTime: int, DepTime: int, DepDelay: double, DepDelayMinutes: double, DepDel15: double, DepartureDelayGroups: int, DepTimeBlk: string, TaxiOut: double, WheelsOff: int, WheelsOn: int, TaxiIn: double, CRSArrTime: int, ArrTime: int, ArrDelay: double, ArrDelayMinutes: double, ArrDel15: double, ArrivalDelayGroups: int, ArrTimeBlk: string, Cancelled: double, CancellationCode: string, Diverted: double, CRSElapsedTime: double, ActualElapsedTime: double, AirTime: double, Flights: double, Distance: double, DistanceGroup: int, CarrierDelay: double, WeatherDelay: double, NASDelay: double, SecurityDelay: double, LateAircraftDelay: double, FirstDepTime: int, TotalAddGTime: double, LongestAddGTime: double, DivAirportLandings: int, DivReachedDest: double, DivActualElapsedTime: double, DivArrDelay: double, DivDistance: double, Div1Airport: string, Div1AirportID: int, Div1AirportSeqID: int, Div1WheelsOn: int, Div1TotalGTime: double, Div1LongestGTime: double, Div1WheelsOff: int, Div1TailNum: string, Div2Airport: string, Div2AirportID: int, Div2AirportSeqID: int, Div2WheelsOn: int, Div2TotalGTime: double, Div2LongestGTime: double, Div2WheelsOff: int, Div2TailNum: string, Div3Airport: string, Div3AirportID: string, Div3AirportSeqID: string, Div3WheelsOn: string, Div3TotalGTime: string, Div3LongestGTime: string, Div3WheelsOff: string, Div3TailNum: string, Div4Airport: string, Div4AirportID: string, Div4AirportSeqID: string, Div4WheelsOn: string, Div4TotalGTime: string, Div4LongestGTime: string, Div4WheelsOff: string, Div4TailNum: string, Div5Airport: string, Div5AirportID: string, Div5AirportSeqID: string, Div5WheelsOn: string, Div5TotalGTime: string, Div5LongestGTime: string, Div5WheelsOff: string, Div5TailNum: string, _c109: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy2_Kw6PLbeU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca013f60-9b19-4e11-be69-e5669cba8fe5"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Year: integer (nullable = true)\n",
            " |-- Quarter: integer (nullable = true)\n",
            " |-- Month: integer (nullable = true)\n",
            " |-- DayofMonth: integer (nullable = true)\n",
            " |-- DayOfWeek: integer (nullable = true)\n",
            " |-- FlightDate: timestamp (nullable = true)\n",
            " |-- Reporting_Airline: string (nullable = true)\n",
            " |-- DOT_ID_Reporting_Airline: integer (nullable = true)\n",
            " |-- IATA_CODE_Reporting_Airline: string (nullable = true)\n",
            " |-- Tail_Number: string (nullable = true)\n",
            " |-- Flight_Number_Reporting_Airline: integer (nullable = true)\n",
            " |-- OriginAirportID: integer (nullable = true)\n",
            " |-- OriginAirportSeqID: integer (nullable = true)\n",
            " |-- OriginCityMarketID: integer (nullable = true)\n",
            " |-- Origin: string (nullable = true)\n",
            " |-- OriginCityName: string (nullable = true)\n",
            " |-- OriginState: string (nullable = true)\n",
            " |-- OriginStateFips: integer (nullable = true)\n",
            " |-- OriginStateName: string (nullable = true)\n",
            " |-- OriginWac: integer (nullable = true)\n",
            " |-- DestAirportID: integer (nullable = true)\n",
            " |-- DestAirportSeqID: integer (nullable = true)\n",
            " |-- DestCityMarketID: integer (nullable = true)\n",
            " |-- Dest: string (nullable = true)\n",
            " |-- DestCityName: string (nullable = true)\n",
            " |-- DestState: string (nullable = true)\n",
            " |-- DestStateFips: integer (nullable = true)\n",
            " |-- DestStateName: string (nullable = true)\n",
            " |-- DestWac: integer (nullable = true)\n",
            " |-- CRSDepTime: integer (nullable = true)\n",
            " |-- DepTime: integer (nullable = true)\n",
            " |-- DepDelay: double (nullable = true)\n",
            " |-- DepDelayMinutes: double (nullable = true)\n",
            " |-- DepDel15: double (nullable = true)\n",
            " |-- DepartureDelayGroups: integer (nullable = true)\n",
            " |-- DepTimeBlk: string (nullable = true)\n",
            " |-- TaxiOut: double (nullable = true)\n",
            " |-- WheelsOff: integer (nullable = true)\n",
            " |-- WheelsOn: integer (nullable = true)\n",
            " |-- TaxiIn: double (nullable = true)\n",
            " |-- CRSArrTime: integer (nullable = true)\n",
            " |-- ArrTime: integer (nullable = true)\n",
            " |-- ArrDelay: double (nullable = true)\n",
            " |-- ArrDelayMinutes: double (nullable = true)\n",
            " |-- ArrDel15: double (nullable = true)\n",
            " |-- ArrivalDelayGroups: integer (nullable = true)\n",
            " |-- ArrTimeBlk: string (nullable = true)\n",
            " |-- Cancelled: double (nullable = true)\n",
            " |-- CancellationCode: string (nullable = true)\n",
            " |-- Diverted: double (nullable = true)\n",
            " |-- CRSElapsedTime: double (nullable = true)\n",
            " |-- ActualElapsedTime: double (nullable = true)\n",
            " |-- AirTime: double (nullable = true)\n",
            " |-- Flights: double (nullable = true)\n",
            " |-- Distance: double (nullable = true)\n",
            " |-- DistanceGroup: integer (nullable = true)\n",
            " |-- CarrierDelay: double (nullable = true)\n",
            " |-- WeatherDelay: double (nullable = true)\n",
            " |-- NASDelay: double (nullable = true)\n",
            " |-- SecurityDelay: double (nullable = true)\n",
            " |-- LateAircraftDelay: double (nullable = true)\n",
            " |-- FirstDepTime: integer (nullable = true)\n",
            " |-- TotalAddGTime: double (nullable = true)\n",
            " |-- LongestAddGTime: double (nullable = true)\n",
            " |-- DivAirportLandings: integer (nullable = true)\n",
            " |-- DivReachedDest: double (nullable = true)\n",
            " |-- DivActualElapsedTime: double (nullable = true)\n",
            " |-- DivArrDelay: double (nullable = true)\n",
            " |-- DivDistance: double (nullable = true)\n",
            " |-- Div1Airport: string (nullable = true)\n",
            " |-- Div1AirportID: integer (nullable = true)\n",
            " |-- Div1AirportSeqID: integer (nullable = true)\n",
            " |-- Div1WheelsOn: integer (nullable = true)\n",
            " |-- Div1TotalGTime: double (nullable = true)\n",
            " |-- Div1LongestGTime: double (nullable = true)\n",
            " |-- Div1WheelsOff: integer (nullable = true)\n",
            " |-- Div1TailNum: string (nullable = true)\n",
            " |-- Div2Airport: string (nullable = true)\n",
            " |-- Div2AirportID: integer (nullable = true)\n",
            " |-- Div2AirportSeqID: integer (nullable = true)\n",
            " |-- Div2WheelsOn: integer (nullable = true)\n",
            " |-- Div2TotalGTime: double (nullable = true)\n",
            " |-- Div2LongestGTime: double (nullable = true)\n",
            " |-- Div2WheelsOff: integer (nullable = true)\n",
            " |-- Div2TailNum: string (nullable = true)\n",
            " |-- Div3Airport: string (nullable = true)\n",
            " |-- Div3AirportID: string (nullable = true)\n",
            " |-- Div3AirportSeqID: string (nullable = true)\n",
            " |-- Div3WheelsOn: string (nullable = true)\n",
            " |-- Div3TotalGTime: string (nullable = true)\n",
            " |-- Div3LongestGTime: string (nullable = true)\n",
            " |-- Div3WheelsOff: string (nullable = true)\n",
            " |-- Div3TailNum: string (nullable = true)\n",
            " |-- Div4Airport: string (nullable = true)\n",
            " |-- Div4AirportID: string (nullable = true)\n",
            " |-- Div4AirportSeqID: string (nullable = true)\n",
            " |-- Div4WheelsOn: string (nullable = true)\n",
            " |-- Div4TotalGTime: string (nullable = true)\n",
            " |-- Div4LongestGTime: string (nullable = true)\n",
            " |-- Div4WheelsOff: string (nullable = true)\n",
            " |-- Div4TailNum: string (nullable = true)\n",
            " |-- Div5Airport: string (nullable = true)\n",
            " |-- Div5AirportID: string (nullable = true)\n",
            " |-- Div5AirportSeqID: string (nullable = true)\n",
            " |-- Div5WheelsOn: string (nullable = true)\n",
            " |-- Div5TotalGTime: string (nullable = true)\n",
            " |-- Div5LongestGTime: string (nullable = true)\n",
            " |-- Div5WheelsOff: string (nullable = true)\n",
            " |-- Div5TailNum: string (nullable = true)\n",
            " |-- _c109: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mNrF_ZjHL5p",
        "colab_type": "text"
      },
      "source": [
        "## Select relevant columns\n",
        "\n",
        "Literally the same syntax as Pandas!\n",
        "\n",
        "```python\n",
        "df = df.select(['FlightDate', 'DayOfWeek', 'Reporting_Airline', 'Tail_Number', 'Flight_Number_Reporting_Airline', 'Origin', \n",
        "                'OriginCityName', 'OriginStateName', 'Dest', 'DestCityName', 'DestStateName',\n",
        "                'DepTime', 'DepDelay', 'AirTime', 'Distance'])\n",
        "\n",
        "df\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdGKIsRVMQT7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "e70250c5-5c35-4871-d5ae-df2536c3682b"
      },
      "source": [
        "!head -n 2 On_Time_Reporting_Carrier_On_Time_Performance_\\(1987_present\\)_2018_12.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"Year\",\"Quarter\",\"Month\",\"DayofMonth\",\"DayOfWeek\",\"FlightDate\",\"Reporting_Airline\",\"DOT_ID_Reporting_Airline\",\"IATA_CODE_Reporting_Airline\",\"Tail_Number\",\"Flight_Number_Reporting_Airline\",\"OriginAirportID\",\"OriginAirportSeqID\",\"OriginCityMarketID\",\"Origin\",\"OriginCityName\",\"OriginState\",\"OriginStateFips\",\"OriginStateName\",\"OriginWac\",\"DestAirportID\",\"DestAirportSeqID\",\"DestCityMarketID\",\"Dest\",\"DestCityName\",\"DestState\",\"DestStateFips\",\"DestStateName\",\"DestWac\",\"CRSDepTime\",\"DepTime\",\"DepDelay\",\"DepDelayMinutes\",\"DepDel15\",\"DepartureDelayGroups\",\"DepTimeBlk\",\"TaxiOut\",\"WheelsOff\",\"WheelsOn\",\"TaxiIn\",\"CRSArrTime\",\"ArrTime\",\"ArrDelay\",\"ArrDelayMinutes\",\"ArrDel15\",\"ArrivalDelayGroups\",\"ArrTimeBlk\",\"Cancelled\",\"CancellationCode\",\"Diverted\",\"CRSElapsedTime\",\"ActualElapsedTime\",\"AirTime\",\"Flights\",\"Distance\",\"DistanceGroup\",\"CarrierDelay\",\"WeatherDelay\",\"NASDelay\",\"SecurityDelay\",\"LateAircraftDelay\",\"FirstDepTime\",\"TotalAddGTime\",\"LongestAddGTime\",\"DivAirportLandings\",\"DivReachedDest\",\"DivActualElapsedTime\",\"DivArrDelay\",\"DivDistance\",\"Div1Airport\",\"Div1AirportID\",\"Div1AirportSeqID\",\"Div1WheelsOn\",\"Div1TotalGTime\",\"Div1LongestGTime\",\"Div1WheelsOff\",\"Div1TailNum\",\"Div2Airport\",\"Div2AirportID\",\"Div2AirportSeqID\",\"Div2WheelsOn\",\"Div2TotalGTime\",\"Div2LongestGTime\",\"Div2WheelsOff\",\"Div2TailNum\",\"Div3Airport\",\"Div3AirportID\",\"Div3AirportSeqID\",\"Div3WheelsOn\",\"Div3TotalGTime\",\"Div3LongestGTime\",\"Div3WheelsOff\",\"Div3TailNum\",\"Div4Airport\",\"Div4AirportID\",\"Div4AirportSeqID\",\"Div4WheelsOn\",\"Div4TotalGTime\",\"Div4LongestGTime\",\"Div4WheelsOff\",\"Div4TailNum\",\"Div5Airport\",\"Div5AirportID\",\"Div5AirportSeqID\",\"Div5WheelsOn\",\"Div5TotalGTime\",\"Div5LongestGTime\",\"Div5WheelsOff\",\"Div5TailNum\",\n",
            "2018,4,12,25,2,2018-12-25,\"WN\",19393,\"WN\",\"N566WN\",\"1823\",13796,1379608,32457,\"OAK\",\"Oakland, CA\",\"CA\",\"06\",\"California\",91,11884,1188402,31884,\"GEG\",\"Spokane, WA\",\"WA\",\"53\",\"Washington\",93,\"1030\",\"1048\",18.00,18.00,1.00,1,\"1000-1059\",12.00,\"1100\",\"1251\",4.00,\"1230\",\"1255\",25.00,25.00,1.00,1,\"1200-1259\",0.00,\"\",0.00,120.00,127.00,111.00,1.00,723.00,3,18.00,0.00,7.00,0.00,0.00,\"\",,,0,,,,,\"\",,,\"\",,,\"\",\"\",\"\",,,\"\",,,\"\",\"\",\"\",,,\"\",,,\"\",\"\",\"\",,,\"\",,,\"\",\"\",\"\",,,\"\",,,\"\",\"\",\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "abNW7p2RHL5u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ba1f87e8-44d6-4e33-cdf6-234de0d0839a"
      },
      "source": [
        "df = df[['FlightDate', 'DayOfWeek', 'Reporting_Airline', 'Tail_Number', 'Flight_Number_Reporting_Airline', 'Origin', \n",
        "                'OriginCityName', 'OriginStateName', 'Dest', 'DestCityName', 'DestStateName',\n",
        "                'DepTime', 'DepDelay', 'AirTime', 'Distance']]\n",
        " \n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[FlightDate: timestamp, DayOfWeek: int, Reporting_Airline: string, Tail_Number: string, Flight_Number_Reporting_Airline: int, Origin: string, OriginCityName: string, OriginStateName: string, Dest: string, DestCityName: string, DestStateName: string, DepTime: int, DepDelay: double, AirTime: double, Distance: double]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX94bCxKHL6M",
        "colab_type": "text"
      },
      "source": [
        "### Extract \"Hour\" variable\n",
        "\n",
        "The DepTimes have been inferred to be floats. We need them as ints, representing each o fthe 24 hours in a day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU94wQlRHL6S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "d4f5b756-e293-4f40-b225-e7350f55d36e"
      },
      "source": [
        "df[['DepTime']].show(6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+\n",
            "|DepTime|\n",
            "+-------+\n",
            "|   1048|\n",
            "|    638|\n",
            "|   1710|\n",
            "|   1318|\n",
            "|    953|\n",
            "|   1646|\n",
            "+-------+\n",
            "only showing top 6 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpZFJA-QO1b2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "cc771f4b-568b-4577-cb71-7cb4ae2299d9"
      },
      "source": [
        "from pyspark.sql import functions as f\n",
        "\n",
        "def hour_str(hour):\n",
        "  return str(hour)[:-2]\n",
        "\n",
        "hour_str(755)\n",
        "\n",
        "hour_str = f.udf(lambda hour: str(hour)[:-2])\n",
        "df.select(hour_str('DepTime')).show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+\n",
            "|<lambda>(DepTime)|\n",
            "+-----------------+\n",
            "|               10|\n",
            "|                6|\n",
            "|               17|\n",
            "|               13|\n",
            "|                9|\n",
            "+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YO0mP8dhP0ac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "31b7dd94-5935-49a9-a19c-c68fe1824802"
      },
      "source": [
        "df.select('DepTime')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[DepTime: int]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEw0UTdlP28c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26beae49-d8a4-4baa-cfbb-6c823b71daba"
      },
      "source": [
        "df['DepTime']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Column<b'DepTime'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivuvmHXJPczE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "b29950bd-f3ac-4151-e3e1-077662d051e4"
      },
      "source": [
        "from pyspark.sql import types\n",
        "\n",
        "df.select(df['DepTime'].astype(types.StringType())[0:-2]).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------+\n",
            "|substring(CAST(DepTime AS STRING), 0, -2)|\n",
            "+-----------------------------------------+\n",
            "|                                         |\n",
            "|                                         |\n",
            "|                                         |\n",
            "|                                         |\n",
            "|                                         |\n",
            "|                                         |\n",
            "|                                         |\n",
            "|                                         |\n",
            "|                                         |\n",
            "|                                         |\n",
            "|                                         |\n",
            "|                                         |\n",
            "|                                         |\n",
            "|                                         |\n",
            "|                                         |\n",
            "|                                         |\n",
            "|                                         |\n",
            "|                                         |\n",
            "|                                         |\n",
            "|                                         |\n",
            "+-----------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVBf8EG8Q1y9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2 = df.withColumn('Hour', (df['DepTime'] / 100).cast(types.IntegerType()))\n",
        "df2.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqGuoechHL6s",
        "colab_type": "text"
      },
      "source": [
        "## Generate the relative distributions\n",
        "\n",
        "In order to be able to handle the data, we need to reduce its dimensionality. Since we want to describe a discrete distribution, we can just count how many values of each level of the 'DepDelay' variable we find for each hour (24 different discrete distributions). We also want the totals in order to do the relative distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAWanzMYHL6x",
        "colab_type": "text"
      },
      "source": [
        "### Totals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-XduNiTHL64",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "9bfe8409-d113-4ac7-a545-92c0388a8c80"
      },
      "source": [
        "totals_per_hour = df2.groupby('Hour').count()\n",
        "totals_per_hour.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+\n",
            "|Hour|count|\n",
            "+----+-----+\n",
            "|  12|36925|\n",
            "|  22|15646|\n",
            "|null| 6526|\n",
            "|   1|  846|\n",
            "|  13|33163|\n",
            "|   6|37398|\n",
            "|  16|34761|\n",
            "|   3|  200|\n",
            "|  20|28354|\n",
            "|   5|22597|\n",
            "|  19|30966|\n",
            "|  15|35691|\n",
            "|  17|36591|\n",
            "|   9|34355|\n",
            "|   4| 1374|\n",
            "|   8|36357|\n",
            "|  23| 5835|\n",
            "|   7|35336|\n",
            "|  10|34600|\n",
            "|  24|   40|\n",
            "+----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvObP4vhHL7V",
        "colab_type": "text"
      },
      "source": [
        "### Distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ6jtyGRHL7c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "9c2ebd32-19c6-40a7-b295-864430fc026b"
      },
      "source": [
        "per_delay_per_hour = df2.groupBy(['Hour', 'DepDelay']).count()\n",
        "per_delay_per_hour.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+--------+-----+\n",
            "|Hour|DepDelay|count|\n",
            "+----+--------+-----+\n",
            "|  17|     8.0|  388|\n",
            "|  13|    19.0|  197|\n",
            "|  22|     9.0|  158|\n",
            "|  16|    76.0|   31|\n",
            "|  17|    72.0|   32|\n",
            "|   6|    89.0|    4|\n",
            "|  23|   108.0|   14|\n",
            "|  22|    73.0|   24|\n",
            "|  16|    84.0|   33|\n",
            "|  18|   -11.0|  435|\n",
            "|  17|    86.0|   21|\n",
            "|   0|    95.0|   10|\n",
            "|  10|    70.0|   18|\n",
            "|  20|   173.0|    9|\n",
            "|   7|    52.0|   22|\n",
            "|  20|   226.0|    4|\n",
            "|   4|    -8.0|   74|\n",
            "|   8|   178.0|    2|\n",
            "|  13|   211.0|    2|\n",
            "|  22|   383.0|    1|\n",
            "+----+--------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMlW_pj7HL75",
        "colab_type": "text"
      },
      "source": [
        "Now we join both and calculate what fraction of the total for each hour each level of DepDelay represents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJxSHV9JHL79",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "d1146983-fd02-472f-8491-d9bbee7314ce"
      },
      "source": [
        "with_totals = per_delay_per_hour.join(totals_per_hour, on='Hour')\n",
        "with_totals.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+--------+-----+-----+\n",
            "|Hour|DepDelay|count|count|\n",
            "+----+--------+-----+-----+\n",
            "|  17|     8.0|  388|36591|\n",
            "|  13|    19.0|  197|33163|\n",
            "|  22|     9.0|  158|15646|\n",
            "|  16|    76.0|   31|34761|\n",
            "|  17|    72.0|   32|36591|\n",
            "|   6|    89.0|    4|37398|\n",
            "|  23|   108.0|   14| 5835|\n",
            "|  22|    73.0|   24|15646|\n",
            "|  16|    84.0|   33|34761|\n",
            "|  18|   -11.0|  435|33335|\n",
            "|  17|    86.0|   21|36591|\n",
            "|   0|    95.0|   10| 2283|\n",
            "|  10|    70.0|   18|34600|\n",
            "|  20|   173.0|    9|28354|\n",
            "|   7|    52.0|   22|35336|\n",
            "|  20|   226.0|    4|28354|\n",
            "|   4|    -8.0|   74| 1374|\n",
            "|   8|   178.0|    2|36357|\n",
            "|  13|   211.0|    2|33163|\n",
            "|  22|   383.0|    1|15646|\n",
            "+----+--------+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-2dlWfybqXH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "outputId": "21900f3e-7532-477e-bee5-bc0e7bce0c74"
      },
      "source": [
        "with_totals['count'] / with_totals['count']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o550.apply.\n: org.apache.spark.sql.AnalysisException: Reference 'count' is ambiguous, could be: count, count.;\n\tat org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:259)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveQuoted(LogicalPlan.scala:121)\n\tat org.apache.spark.sql.Dataset.resolve(Dataset.scala:221)\n\tat org.apache.spark.sql.Dataset.col(Dataset.scala:1274)\n\tat org.apache.spark.sql.Dataset.apply(Dataset.scala:1241)\n\tat sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-8b19e018dc8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwith_totals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mwith_totals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \"\"\"\n\u001b[1;32m   1283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: \"Reference 'count' is ambiguous, could be: count, count.;\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYV4Ixm-cEuC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "becdec56-2f8d-4386-b9f0-a1ef0bc083a7"
      },
      "source": [
        "with_totals.select(per_delay_per_hour['count'] / totals_per_hour['count'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[(count / count): double]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMjbRuYVcRtL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16b643bf-0764-473b-ff5b-5aad69b71012"
      },
      "source": [
        "relative_freqs = with_totals.withColumn('relative', per_delay_per_hour['count'] / totals_per_hour['count'])\n",
        "relative_freqs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Hour: int, DepDelay: double, count: bigint, count: bigint, relative: double]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "greYnaj0cbLA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "51004183-dfc5-4d59-c940-2d0ed138843e"
      },
      "source": [
        "relative_freqs[relative_freqs['Hour']==20].show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+--------+-----+-----+--------------------+\n",
            "|Hour|DepDelay|count|count|            relative|\n",
            "+----+--------+-----+-----+--------------------+\n",
            "|  20|   173.0|    9|28354|3.174155322000423E-4|\n",
            "|  20|   226.0|    4|28354|1.410735698666854...|\n",
            "|  20|   530.0|    1|28354|3.526839246667136...|\n",
            "|  20|    82.0|   32|28354|0.001128588558933...|\n",
            "|  20|   218.0|    3|28354|1.058051774000141E-4|\n",
            "|  20|   205.0|    4|28354|1.410735698666854...|\n",
            "|  20|   237.0|    2|28354|7.053678493334273E-5|\n",
            "|  20|   103.0|   19|28354| 6.70099456866756E-4|\n",
            "|  20|   160.0|   11|28354|3.879523171333850...|\n",
            "|  20|   224.0|    2|28354|7.053678493334273E-5|\n",
            "|  20|   343.0|    3|28354|1.058051774000141E-4|\n",
            "|  20|   118.0|   24|28354|8.464414192001129E-4|\n",
            "|  20|   449.0|    1|28354|3.526839246667136...|\n",
            "|  20|   465.0|    1|28354|3.526839246667136...|\n",
            "|  20|    71.0|   31|28354|0.001093320166466...|\n",
            "|  20|   -20.0|   19|28354| 6.70099456866756E-4|\n",
            "|  20|   107.0|   15|28354|5.290258870000705E-4|\n",
            "|  20|    85.0|   29|28354|0.001022783381533...|\n",
            "|  20|    53.0|   46|28354|0.001622346053466883|\n",
            "|  20|   243.0|    4|28354|1.410735698666854...|\n",
            "+----+--------+-----+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeCVTayyHL8W",
        "colab_type": "text"
      },
      "source": [
        "### Generate distributions\n",
        "\n",
        "We have to group on the hour. Each group will be a bunch of delays and the corresponding frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK84SxcKHL8a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "6c8aa7ee-6b18-4a01-8dc8-f278555d30a4"
      },
      "source": [
        "distributions = relative_freqs.groupby('Hour').agg(f.collect_list('relative'), f.collect_list('DepDelay')).cache()\n",
        "distributions.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----------------------+----------------------+\n",
            "|Hour|collect_list(relative)|collect_list(DepDelay)|\n",
            "+----+----------------------+----------------------+\n",
            "|  12|  [2.70819228165199...|  [1315.0, 280.0, 3...|\n",
            "|  22|  [0.01009842771315...|  [9.0, 73.0, 383.0...|\n",
            "|   1|  [0.00118203309692...|  [270.0, 223.0, 30...|\n",
            "|  13|  [0.00594035521514...|  [19.0, 211.0, -18...|\n",
            "|  16|  [8.91804033255660...|  [76.0, 84.0, 289....|\n",
            "|   6|  [1.06957591315043...|  [89.0, 678.0, 540...|\n",
            "|   3|  [0.005, 0.01, 0.0...|  [157.0, 11.0, 139...|\n",
            "|  20|  [3.17415532200042...|  [173.0, 226.0, 53...|\n",
            "|   5|  [4.42536619905297...|  [64.0, 42.0, -19....|\n",
            "|  19|  [2.90641348575857...|  [131.0, 330.0, 14...|\n",
            "|  15|  [2.80182679106777...|  [238.0, 47.0, 584...|\n",
            "|  17|  [0.01060370036347...|  [8.0, 72.0, 86.0,...|\n",
            "|   9|  [5.82156891282200...|  [170.0, 60.0, 67....|\n",
            "|   4|  [0.05385735080058...|  [-8.0, 3.0, 24.0,...|\n",
            "|   8|  [5.50100393321781...|  [178.0, 1236.0, 1...|\n",
            "|  23|  [0.00239931448157...|  [108.0, 129.0, 23...|\n",
            "|   7|  [6.22594521168213...|  [52.0, 91.0, 51.0...|\n",
            "|  10|  [5.20231213872832...|  [70.0, 287.0, 332...|\n",
            "|  24|  [0.05, 0.025, 0.0...|  [-7.0, 80.0, -5.0...|\n",
            "|  21|  [4.72987176792095...|  [-21.0, 307.0, 21...|\n",
            "+----+----------------------+----------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi-0JYnpHL80",
        "colab_type": "text"
      },
      "source": [
        "These groups are definitely manageable: the number of levels will be on the order of a few hundreds to a couple thousands. We can combine them into lists straight away."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEYSQMtsHL83",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "90fc0f95-6ea0-4a0a-deca-0a8456cb611e"
      },
      "source": [
        "rels = [.05, 0.14, .2, .07, .15, .31, .08]\n",
        "delays = [ 0, 2, -3, 1, -5, 3, 4 ]\n",
        "\n",
        "def quartiles(rels, delays):\n",
        "\n",
        "  accumulated = 0\n",
        "  result = []\n",
        "\n",
        "  for delay, relative in sorted(zip(delays, rels)):\n",
        "    prev = accumulated\n",
        "    accumulated += relative\n",
        "\n",
        "    if prev == 0 and accumulated >= 0:\n",
        "      result.append(delay)\n",
        "    if prev < .25 and accumulated >= .25:\n",
        "      result.append(delay)\n",
        "    if prev < .5 and accumulated >= .5:\n",
        "      result.append(delay)\n",
        "    if prev < .75 and accumulated >= .75:\n",
        "      result.append(delay)\n",
        "\n",
        "  result.append(delay)\n",
        "  \n",
        "  return result\n",
        "\n",
        "quartiles(rels, delays)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-5, -3, 2, 3, 4]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0OhWb5IHL9T",
        "colab_type": "text"
      },
      "source": [
        "Now it's be easy to use a UDF to merge the two lists and sort them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pQwFiBoHL9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOBl8x4xHL9v",
        "colab_type": "text"
      },
      "source": [
        "Careful! If we keep that string return type, it might be problematic later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPqn731cHL9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oEUoXUxHL-N",
        "colab_type": "text"
      },
      "source": [
        "### Calculating the quartiles\n",
        "\n",
        "We are finally ready to calculate the quartiles! We will use a UDF.\n",
        "\n",
        "The input to our custom function will be one of the distributions coded like we did: as a list of tuples `(value, relative_frequency)`. The quartiles are defined as the values at which we cross the 0.0, .25, .5, .75 and 1.00 relative frequencies. Since the distributions are ordered, we can just iterate over one while keeping track of what portion of the total distribution we have seen, and annotate where we cross the thresholds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93lakO_XHL-R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "69f2f5f0-c55f-494c-bbb6-97604bb9930e"
      },
      "source": [
        "quartiles_udf = f.udf(quartiles)\n",
        "\n",
        "distributions.withColumn('quartiles', quartiles_udf('collect_list(relative)', 'collect_list(DepDelay)')).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----------------------+----------------------+--------------------+\n",
            "|Hour|collect_list(relative)|collect_list(DepDelay)|           quartiles|\n",
            "+----+----------------------+----------------------+--------------------+\n",
            "|  12|  [2.70819228165199...|  [1315.0, 280.0, 3...|[-38.0, -5.0, -2....|\n",
            "|  22|  [0.01009842771315...|  [9.0, 73.0, 383.0...|[-39.0, -5.0, 0.0...|\n",
            "|   1|  [0.00118203309692...|  [270.0, 223.0, 30...|[-22.0, -3.0, 13....|\n",
            "|  13|  [0.00594035521514...|  [19.0, 211.0, -18...|[-32.0, -5.0, -2....|\n",
            "|  16|  [8.91804033255660...|  [76.0, 84.0, 289....|[-37.0, -5.0, -1....|\n",
            "|   6|  [1.06957591315043...|  [89.0, 678.0, 540...|[-32.0, -6.0, -3....|\n",
            "|   3|  [0.005, 0.01, 0.0...|  [157.0, 11.0, 139...|[-19.0, -7.0, -1....|\n",
            "|  20|  [3.17415532200042...|  [173.0, 226.0, 53...|[-35.0, -5.0, 0.0...|\n",
            "|   5|  [4.42536619905297...|  [64.0, 42.0, -19....|[-24.0, -7.0, -4....|\n",
            "|  19|  [2.90641348575857...|  [131.0, 330.0, 14...|[-32.0, -5.0, -1....|\n",
            "|  15|  [2.80182679106777...|  [238.0, 47.0, 584...|[-60.0, -5.0, -2....|\n",
            "|  17|  [0.01060370036347...|  [8.0, 72.0, 86.0,...|[-46.0, -5.0, -2....|\n",
            "|   9|  [5.82156891282200...|  [170.0, 60.0, 67....|[-31.0, -6.0, -3....|\n",
            "|   4|  [0.05385735080058...|  [-8.0, 3.0, 24.0,...|[-23.0, -7.0, -5....|\n",
            "|   8|  [5.50100393321781...|  [178.0, 1236.0, 1...|[-72.0, -6.0, -3....|\n",
            "|  23|  [0.00239931448157...|  [108.0, 129.0, 23...|[-30.0, -4.0, 14....|\n",
            "|   7|  [6.22594521168213...|  [52.0, 91.0, 51.0...|[-28.0, -6.0, -3....|\n",
            "|  10|  [5.20231213872832...|  [70.0, 287.0, 332...|[-26.0, -5.0, -2....|\n",
            "|  24|  [0.05, 0.025, 0.0...|  [-7.0, 80.0, -5.0...|[-7.0, 1.0, 21.0,...|\n",
            "|  21|  [4.72987176792095...|  [-21.0, 307.0, 21...|[-39.0, -5.0, 0.0...|\n",
            "+----+----------------------+----------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O_6IktsHL-p",
        "colab_type": "text"
      },
      "source": [
        "Apply to the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia3NzYrkHL-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha1JwNyCHL_E",
        "colab_type": "text"
      },
      "source": [
        "### Plotting\n",
        "\n",
        "We got it! Let's move this over to Pandas for convenient handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1PYPzV_HL_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xwfOrcYHL_g",
        "colab_type": "text"
      },
      "source": [
        "And we are ready to plot!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FZsat5-HL_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVRnxpnGHL_6",
        "colab_type": "text"
      },
      "source": [
        "## Test your \"Big Data\" prototype with small data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_k3ycxUHL_9",
        "colab_type": "text"
      },
      "source": [
        "### Summary\n",
        "\n",
        "This is the whole process, collected in one place as is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfnL4QOKHMAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdtPtmkuHMAf",
        "colab_type": "text"
      },
      "source": [
        "### Pyspark job\n",
        "\n",
        "In order to run the process in a cluster, we need to transform it into a pyspark job file. \n",
        "\n",
        "We need to tidy up the function definitions, add the relevant imports, and modify the input and output to use command-line arguments.\n",
        "\n",
        "We will put the result in a file called mysparkjob.py:\n",
        "\n",
        "```python\n",
        "from __future__ import print_function\n",
        "from pyspark.sql import types, functions, SparkSession\n",
        "import sys\n",
        "\n",
        "def zipsort(a, b):\n",
        "    return sorted(zip(a, b))\n",
        "\n",
        "def quartiles(histogram):\n",
        "    area = 0\n",
        "    result = []\n",
        "    \n",
        "    for value, percentage in histogram:\n",
        "        if area == 0:\n",
        "            result.append(value)\n",
        "        elif area <= .25 and area + percentage > .25:\n",
        "            result.append(value)\n",
        "        elif area <= .5 and area + percentage > .5:\n",
        "            result.append(value)\n",
        "        elif area <= .75 and area + percentage > .75:\n",
        "            result.append(value)\n",
        "        area += percentage\n",
        "    \n",
        "    result.append(value)\n",
        "    return result\n",
        "\n",
        "if __name__=='__main__':\n",
        "    \n",
        "    file = sys.argv[1]\n",
        "    out = sys.argv[2]\n",
        "    \n",
        "    spark = SparkSession.builder.getOrCreate()\n",
        "    df = spark.read.csv(file, header= True, inferSchema=True)\n",
        "    df = df.select(['FlightDate', 'DayOfWeek', 'Reporting_Airline', 'Tail_Number', 'Flight_Number_Reporting_Airline', 'Origin', \n",
        "                    'OriginCityName', 'OriginStateName', 'Dest', 'DestCityName', 'DestStateName',\n",
        "                    'DepTime', 'DepDelay', 'AirTime', 'Distance'])\n",
        "\n",
        "    df2 = df.withColumn('Hour', (df['DepTime'] / 100).cast(types.IntegerType()))\n",
        "    totals = df2.groupBy('Hour').count()\n",
        "    distributions = df2.groupBy(['Hour', 'DepDelay']).count()\n",
        "    annotated = distributions.join(totals, on='Hour')\n",
        "    frequencies = annotated.withColumn('relative', distributions['count'] / totals['count'])\n",
        "    groups = frequencies.groupBy(totals['Hour'])\\\n",
        "                        .agg(functions.collect_list('DepDelay').alias('delays'),\n",
        "                             functions.collect_list('relative').alias('relatives'))\n",
        "\n",
        "\n",
        "\n",
        "    zipsort_typed = functions.udf(zipsort, types.ArrayType(types.ArrayType(types.FloatType())))\n",
        "    distributions = groups.withColumn('distributions', zipsort_typed('delays', 'relatives'))\n",
        "\n",
        "\n",
        "\n",
        "    quartiles_udf = functions.udf(quartiles, returnType=types.ArrayType(types.FloatType()))\n",
        "\n",
        "    result = distributions.select('Hour',\n",
        "                                  quartiles_udf('distributions').alias('quartiles'))\n",
        "\n",
        "    result.write.json(out)\n",
        "    spark.stop()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztuuLRW0HMAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lToW3sIGHMA4",
        "colab_type": "text"
      },
      "source": [
        "### Running with spark-submit\n",
        "\n",
        "If the following works, we are ready to test it in the cluster!\n",
        "\n",
        "```python\n",
        "unset PYSPARK_DRIVER_PYTHON\n",
        "spark-submit mysparkjob.py On_Time_On_Time_Performance_2015_8.csv out.csv\n",
        "```"
      ]
    }
  ]
}