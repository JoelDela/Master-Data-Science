{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with computing clusters\n",
    "\n",
    "  <a href = \"http://yogen.io\"><img src=\"http://yogen.io/assets/logo.svg\" alt=\"yogen\" style=\"width: 200px; float: right;\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The idea here is to transmit the idea of how a Data Scientist works remotely.\n",
    "\n",
    "We do it all the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Amazon Web Services\n",
    "\n",
    "![AWS](https://d7umqicpi7263.cloudfront.net/img/product/0f7858eb-0831-4a33-9af9-8e78db6b23d8/c7939ac3-d352-4bee-bf8a-7ee4a2dd2bff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "AWS is widely used\n",
    "\n",
    "You can use it for free\n",
    "\n",
    "Our Spark cluster is based on it, so we'll use free aws machines as a test platform to connect to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Signing up for aws\n",
    "\n",
    "https://aws.amazon.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### AWS services of interest:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- EC2: Elastic Cloud Compute. Allows us to rent virtual machines, computing power, fit to our needs and under several pricing models.\n",
    "\n",
    "- S3: Simple Storage Service: Allows us to rent storage capacity to plug into our virtual servers.\n",
    "\n",
    "- EMR: Elastic MapReduce. Simplifies the creation and management of Hadoop/Spark clusters. \n",
    "\n",
    "- Lambda: Serverless computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating a single instance: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- choosing an operating system\n",
    "\n",
    "- choosing an instance type - spot prices\n",
    "\n",
    "- auto scaling groups\n",
    "\n",
    "- creating a new keypair and storing the private key in .ssh/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating my AWS AMS \n",
    "Ubuntu Server 18.04 LTS (HVM), SSD Volume Type \n",
    "\n",
    "Para conectarte: ssh -i AWS_kschool_example.pem.txt -C ubuntu@35.156.21.82\n",
    "\n",
    "Wolas-MacBook-Pro:GitRepos wola$ ssh -i AWS_kschool_example.pem.txt -C ubuntu@35.156.21.82\n",
    "The authenticity of host '35.156.21.82 (35.156.21.82)' can't be established.\n",
    "ECDSA key fingerprint is SHA256:fGfLRA261rM+m35czyBG7ToHBN04RteK4Ioi11f1Lfo.\n",
    "Are you sure you want to continue connecting (yes/no)? yes\n",
    "Warning: Permanently added '35.156.21.82' (ECDSA) to the list of known hosts.\n",
    "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "@         WARNING: UNPROTECTED PRIVATE KEY FILE!          @\n",
    "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "Permissions 0644 for 'AWS_kschool_example.pem.txt' are too open.\n",
    "It is required that your private key files are NOT accessible by others.\n",
    "This private key will be ignored.\n",
    "Load key \"AWS_kschool_example.pem.txt\": bad permissions\n",
    "Permission denied (publickey).\n",
    "Wolas-MacBook-Pro:GitRepos wola$ ls -l\n",
    "total 8\n",
    "-rw-r--r--@  1 wola  staff  1696 Oct 20 10:24 AWS_kschool_example.pem.txt\n",
    "drwxr-xr-x   8 wola  staff   272 Apr 28 13:04 Coursera_IntroductionToDataScienceInPython\n",
    "drwxr-xr-x   5 wola  staff   170 Oct 14 17:23 KSchool_dataScienceMaster\n",
    "drwxr-xr-x   5 wola  staff   170 Oct 14 17:23 KSchool_dataScienceMaster copy\n",
    "drwxr-xr-x   5 wola  staff   170 Oct 14 17:22 KSchool_dataScienceMaster_bck_141018\n",
    "drwxr-xr-x   8 wola  staff   272 Aug  4 14:18 Malaga\n",
    "drwxr-xr-x  12 wola  staff   408 Oct 19 17:08 PFM_EconomicNewsImpact\n",
    "drwxr-xr-x  20 wola  staff   680 Oct 13 09:54 currency_rates\n",
    "Wolas-MacBook-Pro:GitRepos wola$ chmod 600 AWS_kschool_example.pem.txt \n",
    "Wolas-MacBook-Pro:GitRepos wola$ ssh -i AWS_kschool_example.pem.txt -C ubuntu@35.156.21.82\n",
    "Welcome to Ubuntu 18.04.1 LTS (GNU/Linux 4.15.0-1021-aws x86_64)\n",
    "\n",
    " * Documentation:  https://help.ubuntu.com\n",
    " * Management:     https://landscape.canonical.com\n",
    " * Support:        https://ubuntu.com/advantage\n",
    "\n",
    "  System information as of Sat Oct 20 08:34:56 UTC 2018\n",
    "\n",
    "  System load:  0.0               Processes:           87\n",
    "  Usage of /:   13.3% of 7.69GB   Users logged in:     0\n",
    "  Memory usage: 13%               IP address for eth0: 172.31.35.36\n",
    "  Swap usage:   0%\n",
    "\n",
    "  Get cloud support with Ubuntu Advantage Cloud Guest:\n",
    "    http://www.ubuntu.com/business/services/cloud\n",
    "\n",
    "0 packages can be updated.\n",
    "0 updates are security updates.\n",
    "\n",
    "\n",
    "\n",
    "The programs included with the Ubuntu system are free software;\n",
    "the exact distribution terms for each program are described in the\n",
    "individual files in /usr/share/doc/*/copyright.\n",
    "\n",
    "Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by\n",
    "applicable law.\n",
    "\n",
    "To run a command as administrator (user \"root\"), use \"sudo <command>\".\n",
    "See \"man sudo_root\" for details.\n",
    "\n",
    "ubuntu@ip-172-31-35-36:~$ ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accessing remote computers\n",
    "\n",
    "ssh is your basic tool. You should always use public/private key pair authentication rather than passwords, especially if the ssh port (usually 22) is open to the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ssh`\n",
    "\n",
    "We need to keep the private key (.pem) that we got from amazon somewhere where we will not lose it. The standard place is the `~/.ssh/` folder.\n",
    "\n",
    "```shell\n",
    "mkdir -p ~/.ssh\n",
    "mv $key_file ~/.ssh\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`ssh` will let you control a remote machine as if you were typing at its terminal\n",
    "\n",
    "Let's connect to the instance we just created. \n",
    "\n",
    "We need to use the \"identity file\" (private key) to authenticate ourselves:\n",
    "\n",
    "```shell\n",
    "ssh -i $PRIVATE_KEY -C ec2-user@$INSTANCE_DNS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### scp\n",
    "\n",
    "Sending our data to a remote computer\n",
    "\n",
    "Let's send coupon150720.csv to the recently created instance.\n",
    "\n",
    "```shell\n",
    "scp -i $PRIVATE_KEY $PATH_TO_FILE -C ec2-user@$INSTANCE_DNS:/home/ec2-user\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSH config file\n",
    "\n",
    "An SSH config file saves us from typing those long connections every time. It needs to be in `~/.ssh/config` and looks like this:\n",
    "\n",
    "```\n",
    "Host kschoolcluster\n",
    "    User ec2-user\n",
    "    HostName ec2-52-211-95-201.eu-west-1.compute.amazonaws.com\n",
    "    IdentityFile ~/.ssh/key-cluster-spark-kschool.pem\n",
    "```\n",
    "\n",
    "Once it's there, we can just type\n",
    "\n",
    "```\n",
    "ssh kschoolcluster\n",
    "```\n",
    "and we'll be connected.\n",
    "\n",
    "There are lots and lots of options to ssh we can configure like this. More details [here](https://nerderati.com/2011/03/17/simplify-your-life-with-an-ssh-config-file/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scp coupon150720.csv AWS_example:\n",
    "\n",
    "copia el fichero coupon a mi alias ssh AWS_example\n",
    "\n",
    "y para conectarte, pones ssh AWS_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lambda** te abstrae del entorno. Lo que tu haces es una fncio´n que recibe un evento y, cuando lo recibe, hace algo con el\n",
    "\n",
    "Por ejemplo, un ws de clasificación de imagenes:\n",
    "\n",
    "- Recibe la imagen, aplica el clasificador y devuelve una respuesta.\n",
    "- Cobran por el número de veces que corra\n",
    "- languages (javascript, python, c#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Connecting to the KSchool Spark cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### public-private keys\n",
    "\n",
    "- [ssh-keygen](https://www.ssh.com/ssh/keygen/)\n",
    "\n",
    "\n",
    "```shell\n",
    "ssh-keygen -y -f $PRIVATE_KEY\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We need our public key in `~/.ssh/authorized_keys` in the target machine, that is, the master node.\n",
    "\n",
    "Since you can't connect yet and I am the only one with `sudo`, I'll have to put them myself.\n",
    "\n",
    "Let's each pick one user from kschool20 onwards and do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we can connect to the cluster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```shell\n",
    "TARGET=ec2-52-211-95-201.eu-west-1.compute.amazonaws.com\n",
    "\n",
    "ssh -i $PRIVATE_KEY  -C  $USER@$TARGET\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Storage: HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Assumptions in [HDFS design]: (hadoop distributed file system)\n",
    "\n",
    "* The system is built from many inexpensive commodity components that often fail. \n",
    "\n",
    "* The system stores a modest number of large files. \n",
    "\n",
    "* The workloads primarily consist of two kinds of reads: large streaming reads and small random reads. \n",
    "\n",
    "* The workloads also have many large, sequential writes that append data to files.\n",
    "\n",
    "* The system must efficiently implement well-defined semantics for multiple clients that concurrently append to the same file.\n",
    "\n",
    "* High sustained bandwidth is more important than low latency. \n",
    "\n",
    "[HDFS design]: https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "![HDFS](https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/images/hdfsarchitecture.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### hdfs dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mimics the shell, but with a few differences:\n",
    "\n",
    "* We call shell commands as options to a module named hdfs dfs\n",
    "\n",
    "* There is no concept of a current working directory (therefore, no cd command)\n",
    "\n",
    "* It has some annoying inconsistencies with regular bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```shell\n",
    "\n",
    "[hadoop@masternode ~]$ hdfs dfs \n",
    "\n",
    "Usage: hadoop fs [generic options]\n",
    "\t[-appendToFile <localsrc> ... <dst>]\n",
    "\t[-cat [-ignoreCrc] <src> ...]\n",
    "\t[-checksum <src> ...]\n",
    "\t[-chgrp [-R] GROUP PATH...]\n",
    "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
    "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
    "\t[-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>]\n",
    "\t[-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
    "\t[-count [-q] [-h] <path> ...]\n",
    "\t[-cp [-f] [-p | -p[topax]] <src> ... <dst>]\n",
    "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
    "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
    "\t[-df [-h] [<path> ...]]\n",
    "\t[-du [-s] [-h] <path> ...]\n",
    "\t[-expunge]\n",
    "\t[-find <path> ... <expression> ...]\n",
    "\t[-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
    "\t[-getfacl [-R] <path>]\n",
    "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
    "\t[-getmerge [-nl] <src> <localdst>]\n",
    "\t[-help [cmd ...]]\n",
    "\t[-ls [-d] [-h] [-R] [<path> ...]]\n",
    "\t[-mkdir [-p] <path> ...]\n",
    "\t[-moveFromLocal <localsrc> ... <dst>]\n",
    "\t[-moveToLocal <src> <localdst>]\n",
    "\t[-mv <src> ... <dst>]\n",
    "\t[-put [-f] [-p] [-l] <localsrc> ... <dst>]\n",
    "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
    "\t[-rm [-f] [-r|-R] [-skipTrash] <src> ...]\n",
    "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
    "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
    "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
    "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
    "\t[-stat [format] <path> ...]\n",
    "\t[-tail [-f] <file>]\n",
    "\t[-test -[defsz] <path>]\n",
    "\t[-text [-ignoreCrc] <src> ...]\n",
    "\t[-touchz <path> ...]\n",
    "\t[-truncate [-w] <length> <path> ...]\n",
    "\t[-usage [cmd ...]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Try:\n",
    "\n",
    "```shell\n",
    "user@gateway$ hdfs dfs -ls\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why does it return nothing?\n",
    "\n",
    "Now try:\n",
    "\n",
    "```shell\n",
    "user@gateway$ hdfs dfs -ls /\n",
    "user@gateway$ hdfs dfs -ls /user\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### `hdfs dfs -mkdir`\n",
    "\n",
    "Create a folder inside your hdfs home folder that is called \"data\", on your own\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### `hdfs dfs -put`\n",
    "\n",
    "By analogy with ls, can you guess where the\n",
    "`$LOCAL_FILE` will be put if I do this? (don't do it)\n",
    "                                       \n",
    "```shell\n",
    "user@gateway$ hdfs dfs -put $LOCAL_FILE\n",
    "\n",
    "```\n",
    "                                       \n",
    "                                       \n",
    "Now, put the file in hdfs, inside your \"data\" folder:\n",
    "```shell\n",
    "user@gateway$ hdfs dfs -put $LOCAL_FILE $HDFS_FOLDER\n",
    "```\n",
    " \n",
    "                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### `hdfs dfs -get` / `hdfs dfs -cat`\n",
    "\n",
    "If you do any kind of work in HDFS, eventually you'll need to get something out of it!\n",
    "\n",
    "```shell\n",
    "user@gateway$ hdfs dfs -cat $HDFS_FILE\n",
    "```\n",
    "\n",
    "However, you might only need take a peek into the contents of a file:\n",
    "\n",
    "```shell\n",
    "user@gateway$ hdfs dfs -get $HDFS_FILE\n",
    "```\n",
    "\n",
    "The neat thing about hdfs dfs -cat is that it outputs to stdout, so you can chain it to all your favorite shell pipelines!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Other useful hadoop filesystem commands:\n",
    "    \n",
    "```shell\n",
    "user@gateway$ hdfs dfs -getmerge $HDFS_GLOB $LOCAL_FILE\n",
    "user@gateway$ hdfs dfs -stat $HDFS_FILE\n",
    "user@gateway$ hdfs dfs -tail $HDFS_FILE\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Much more at:\n",
    "https://hadoop.apache.org/docs/r2.8.3/hadoop-project-dist/hadoop-common/FileSystemShell.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## spark-submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### ```mysparkjob.py```\n",
    "\n",
    "Just a simple Spark job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "import sys\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: mysparkjob arg1 arg2 \", file=sys.stderr)\n",
    "        exit(-1)\n",
    "    sc = SparkContext(appName=\"MyTestJob\")\n",
    "    dataTextAll = sc.textFile(sys.argv[1])\n",
    "    dataRDD = dataTextAll.filter(lambda line: line.startswith('79065'))\n",
    "    dataRDD.saveAsTextFile(sys.argv[2])\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Runnning our Spark app\n",
    "\n",
    "```shell\n",
    "./bin/spark-submit \\\n",
    "    mysparkjob.py \\\n",
    "    data/coupon150720.csv \\\n",
    "    test.csv\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantisimo !!!!!!!!\n",
    "Spark sólo lee y escribre de hdfs dfs !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once it runs, what is test.csv? How would you get it back on the local file system?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise \n",
    "\n",
    "Adapt our exercise from notebook 02 to run in the cluster. Remember:\n",
    "\n",
    "Get stats for all tickets with destination MAD from `coupons150720.csv`. You will need to extract ticket amounts with destination MAD, and then calculate:\n",
    "\n",
    "* Total ticket amounts per origin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import types, SparkSession\n",
    "\n",
    "session = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName('My first app in the cluster!')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "raw_coupons = session.read.csv('data/coupon150720.csv')\n",
    "\n",
    "coupons = raw_coupons.select(raw_coupons['_c0'].alias('tkt_number'),\n",
    "                             raw_coupons['_c1'].alias('cpn_number').cast(types.IntegerType()),\n",
    "                             raw_coupons['_c2'].alias('origin'),\n",
    "                             raw_coupons['_c3'].alias('destination'),\n",
    "                             raw_coupons['_c4'].alias('airline'),\n",
    "                             raw_coupons['_c6'].alias('amount').cast(types.FloatType())\n",
    "                            )\n",
    "\n",
    "result = coupons[coupons['destination'] == 'MAD'].groupby('origin').sum('amount')\n",
    "\n",
    "result.write.csv('result_of_job')\n",
    "\n",
    "session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Running on cluster versus client mode\n",
    "\n",
    "This setting controls where the driver runs.\n",
    "\n",
    "The default deployment mode is `client`, that is, the driver runs on the machine that is running the spark-submit script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "```shell\n",
    "./bin/spark-submit \\\n",
    "    mysparkjob.py \\\n",
    "    data/coupon150720.csv \\\n",
    "    --deploy-mode client\n",
    "```\n",
    "\n",
    "\n",
    "```shell\n",
    "./bin/spark-submit \\\n",
    "    mysparkjob.py \\\n",
    "    data/coupon150720.csv \\\n",
    "    --deploy-mode cluster\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "\n",
    "\n",
    "[hadoop fs](https://hadoop.apache.org/docs/r2.8.3/hadoop-project-dist/hadoop-common/FileSystemShell.html)\n",
    "\n",
    "[standalone Spark versus yarn versus Mesos](http://www.agildata.com/apache-spark-cluster-managers-yarn-mesos-or-standalone/)\n",
    "\n",
    "[How Spark runs on clusters](https://spark.apache.org/docs/2.2.0/cluster-overview.html)\n",
    "\n",
    "[spark-submit](https://aws.amazon.com/es/blogs/big-data/submitting-user-applications-with-spark-submit/)\n",
    "\n",
    "[Cluster versus Client deployment modes](https://stackoverflow.com/questions/28807490/what-conditions-should-cluster-deploy-mode-be-used-instead-of-client)\n",
    "\n",
    "[Tunnelling web connections through ssh to view the Spark management web views](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-ssh-tunnel.html)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
