{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second class\n",
    "\n",
    "## Table of Contents (times are approximated)\n",
    "\n",
    "1. [Summary of the first class (0.5-1 h, depending on previous knowledge about OOP)](#summary)  \n",
    "2. [Collaborative Filtering (~15 min)](#cf)  \n",
    "   2.1 [Co-occurrence Matrix (1.5-2h)](#copurchase)\n",
    "   <br></br>\n",
    "   2.2 [Memory-based CF (1-1.5 h)](#memory-base)\n",
    "   <br></br>\n",
    "   2.3 [Model-based CF (2-2.5 h)](#model-base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the first class in code <a id='summary'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class movielens_100k(object):\n",
    "    \"\"\"\n",
    "    This python class read the movielens-100k dataset and:\n",
    "        . Preprocess train and test sets\n",
    "        . Create a python dict that allows us to translate any item id to the corresponding movie title\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_root, item_id_file='u.item', train_file='u2.base', test_file='u2.test', \n",
    "                 columns=['user_id', 'item_id', 'rating', 'timestamp']):\n",
    "                \n",
    "        self.data_root = data_root\n",
    "        self.items_id_file = os.path.join(self.data_root, item_id_file)\n",
    "        self.train_file = train_file\n",
    "        self.test_file = test_file\n",
    "        self.columns = columns\n",
    "        self.__data = {}  # dictionary to store train and test data\n",
    "            \n",
    "    def __getitem__(self, dataset):\n",
    "        \"\"\"\n",
    "        Get train and test data\n",
    "\n",
    "        :param dataset: dataset name (String)\n",
    "        :return: pandas dataframe\n",
    "        \"\"\"\n",
    "        return self.__data[dataset]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        \"\"\"\n",
    "        Store train and test data in the dictionary self.__data.\n",
    "\n",
    "        :param key: name of the dataset (String)\n",
    "        :param value: pandas dataframe\n",
    "        \"\"\"\n",
    "        self.__data[key] = value\n",
    "        \n",
    "    def read_data(self, file, verbose=False):\n",
    "        \"\"\"\n",
    "        Read file returning a pandas dataframe\n",
    "        :param file: file path\n",
    "        :param verbose: print the first rows of the data\n",
    "        \"\"\"\n",
    "        datafile = os.path.join(self.data_root, file)\n",
    "        data = pd.read_csv(datafile, sep='\\t', names=self.columns)\n",
    "        if verbose:\n",
    "            print(data.head())\n",
    "        return data\n",
    "            \n",
    "    def returnItemId(self, text, dict_ids):\n",
    "        \"\"\"\n",
    "        Retrieves all the ids and titles for movies containing 'text' in its title\n",
    "        :param text: string to be looked for in movies titles\n",
    "        :param dict_ids: dicttionary of {id:title}\n",
    "\n",
    "        :return: a list of (id,title) if text found in titles, and an empty list otherwise.\n",
    "        \"\"\"\n",
    "        # convert input text to lowercase\n",
    "        text_ = text.lower()\n",
    "        # find occurances\n",
    "        search = [(k, v.lower().find(text_)) \n",
    "                  for k,v in list(dict_ids.items())]\n",
    "\n",
    "        # Get the IDs corresponding to the given text\n",
    "        index = [k for k,v in search if v>-1]\n",
    "\n",
    "        # Return a list with the id and the name\n",
    "        out = []\n",
    "        for i in index:\n",
    "            out.append((i, dict_ids[i]))\n",
    "        return out\n",
    "    \n",
    "    def check_data_consistency(self):\n",
    "        \"\"\"\n",
    "        Check if one film has two or more ids\n",
    "        \"\"\"\n",
    "        self.item_dict = {}\n",
    "        with io.open(self.items_id_file, 'rb') as f:\n",
    "            for line in f.readlines():\n",
    "                record = line.split(b'|')\n",
    "                self.item_dict[int(record[0])] = str(record[1])\n",
    "                \n",
    "        if len(set(self.item_dict.keys())) == len(set(self.item_dict.values())):\n",
    "            self.unique_item_dict_ = self.item_dict\n",
    "            return True\n",
    "        else:\n",
    "            self.duplicates_item_dict = {}\n",
    "            for id_,name in list(self.item_dict.items()):\n",
    "                if name not in self.duplicates_item_dict:\n",
    "                    self.duplicates_item_dict[name] = [id_]\n",
    "                else:\n",
    "                    self.duplicates_item_dict[name] = self.duplicates_item_dict[name]+[id_]\n",
    "\n",
    "            self.unique_id_item_dict ={}\n",
    "            for i, lista_id in enumerate(self.duplicates_item_dict.values()) :\n",
    "                for key in lista_id:\n",
    "                    self.unique_id_item_dict[key] = i\n",
    "\n",
    "            self.unique_item_dict_ = {self.unique_id_item_dict[k]:v \n",
    "                                for k,v in self.item_dict.items()}\n",
    "            return False\n",
    "    \n",
    "    def correcting_non_unique_movies(self, data_df):\n",
    "        \"\"\"\n",
    "        Preprocess data_df in case the check_data_consistency function fails\n",
    "        \"\"\"\n",
    "        if not self.check_data_consistency():\n",
    "            data_df['item_id'] = data_df['item_id'].apply(lambda x: self.unique_id_item_dict[x])\n",
    "            \n",
    "        return data_df\n",
    "            \n",
    "    def train(self, verbose=False):\n",
    "        \"\"\"\n",
    "        Train data\n",
    "        \"\"\"\n",
    "        self['train_set'] = self.read_data(self.train_file, verbose)\n",
    "        self['train_set'] = self.correcting_non_unique_movies(self['train_set'])\n",
    "        items = self['train_set'].item_id.unique()\n",
    "        self.train_id_dict = {j:i for i,j in enumerate(items)}\n",
    "        self['train_set']['item_id'] = self['train_set']['item_id'].apply(lambda x: self.train_id_dict[x])\n",
    "        self.unique_item_dict = {self.train_id_dict[v]:self.unique_item_dict_[v] \n",
    "                                for v in items}\n",
    "        self.n_users = self['train_set'].user_id.unique().shape[0]\n",
    "        self.n_items = self['train_set'].item_id.unique().shape[0]\n",
    "        self.n_pairs = self['train_set'].shape[0]\n",
    "        if verbose:\n",
    "            print('There are %s users, %s items and %s pairs in the test set' \\\n",
    "                  %(self.n_users, self.n_items, self.n_pairs))\n",
    "    \n",
    "    def test(self, verbose=False):\n",
    "        \"\"\"\n",
    "        Test data\n",
    "        \"\"\"\n",
    "        self['test_set'] = self.read_data(self.test_file, verbose)\n",
    "        self['test_set']['item_id'] = self['test_set']['item_id'].apply(lambda x: self.unique_id_item_dict[x])\n",
    "        self['test_set']['item_id'] = self['test_set']['item_id'].apply(lambda x: self.train_id_dict[x] if x in self.train_id_dict else -1)\n",
    "        self['test_set'] = self['test_set'][self['test_set']['item_id']!=-1]\n",
    "        if verbose:\n",
    "            print('There are %s users, %s items and %s pairs in the test set' \\\n",
    "                  %(self['test_set'].user_id.unique().shape[0], self['test_set'].item_id.unique().shape[0], \n",
    "                    self['test_set'].shape[0]))\n",
    "            \n",
    "    def get_data(self, verbose=False):\n",
    "        \"\"\"\n",
    "        A kind of main function to return train and test data together\n",
    "        \"\"\"\n",
    "        self.train(verbose)\n",
    "        self.test(verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples:\n",
    "movielens = movielens_100k(\"ml-100k/\")\n",
    "movielens.get_data() #Get train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>878542960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>876893119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>889751712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>887431973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>875071561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0        1        0       4  878542960\n",
       "1        1        1       3  876893119\n",
       "2        1        2       3  889751712\n",
       "3        1        3       5  887431973\n",
       "4        1        4       4  875071561"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movielens['train_set'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>289</td>\n",
       "      <td>5</td>\n",
       "      <td>874965758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>290</td>\n",
       "      <td>3</td>\n",
       "      <td>876893171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>373</td>\n",
       "      <td>1</td>\n",
       "      <td>875072484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>374</td>\n",
       "      <td>5</td>\n",
       "      <td>878543541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>665</td>\n",
       "      <td>1</td>\n",
       "      <td>878542772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0        1      289       5  874965758\n",
       "1        1      290       3  876893171\n",
       "2        1      373       1  875072484\n",
       "3        1      374       5  878543541\n",
       "4        1      665       1  878542772"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movielens['test_set'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"b'Get Shorty (1995)'\", \"b'Richard III (1995)'\", \"b'Apollo 13 (1995)'\"]\n"
     ]
    }
   ],
   "source": [
    "print(list(movielens.unique_item_dict[item_id] for item_id in [1,5,20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class popularity_models(object):\n",
    "    \"\"\"\n",
    "    Implement the following popularity models:\n",
    "        . Most rated movie (it is assumed that this is the most watched movie)\n",
    "        . Most positively rated movie (rating > a given threshold)\n",
    "        . Highest rated movie\n",
    "    \"\"\"\n",
    "    def __init__(self, model, train_df):\n",
    "        self.model_name = model\n",
    "        self.train_df = train_df\n",
    "    \n",
    "    def mostRated(self):\n",
    "        \"\"\"\n",
    "        Most rated movie model\n",
    "        \"\"\"\n",
    "        return self.train_df.groupby('item_id')['user_id'].count().sort_values(ascending=False)\n",
    "\n",
    "    def positiveRated(self, min_rating=4.0):\n",
    "        \"\"\"\n",
    "        Most positively rated movie model (rating > min_rating)\n",
    "        \"\"\"\n",
    "        return self.train_df[self.train_df.rating>=min_rating].groupby('item_id')['user_id'].count().sort_values(ascending=False) \n",
    "        \n",
    "    def mean_rate_movies(self, min_ratings=50):\n",
    "        \"\"\"\n",
    "        Highest rated movie model. Only items with more than min_ratings ratings are considered\n",
    "        \"\"\"\n",
    "        listRatedMovies = self.train_df.groupby('item_id')['rating'].apply(list).reset_index()\n",
    "        filteredListRatedMovies = listRatedMovies[listRatedMovies.rating.apply(lambda x: len(x)>min_ratings)]\n",
    "        meanMovies = filteredListRatedMovies.rating.apply(lambda x: np.mean(np.array(x))).sort_values(ascending=False)\n",
    "        return meanMovies\n",
    "        \n",
    "    def train(self, min_rating=4.0, min_ratings=50):\n",
    "        \"\"\"\n",
    "        Train a popularity model (i.e., sort the item list according to a particular frequency metric)\n",
    "        \"\"\"\n",
    "        if self.model_name == 'mostRated':\n",
    "            self.model = self.mostRated()\n",
    "        elif self.model_name == 'positiveRated':\n",
    "            self.model = self.positiveRated(min_rating)\n",
    "        elif self.model_name == 'mean_rate_movies':\n",
    "            self.model = self.mean_rate_movies(min_ratings)\n",
    "        else:\n",
    "            raise ValueError('%s doesnot exist' % self.model_name)\n",
    "        \n",
    "    def predict(self, top_k, verbose=False):\n",
    "        \"\"\"\n",
    "        Return the first top_k recommendation of the chosen popularity model\n",
    "        \"\"\"\n",
    "        return self.model.index.values[:top_k]\n",
    "    \n",
    "    def get_titles(self, movies_ids, unique_item_dict):\n",
    "        \"\"\"\n",
    "        Given a list of movies ids, this method return their titles\n",
    "        \"\"\"\n",
    "        title = []\n",
    "        for i, index in enumerate(movies_ids):\n",
    "            id_ = index # id of the movie\n",
    "            title.append(unique_item_dict[id_])\n",
    "\n",
    "        print(title)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>878542960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>876893119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>889751712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>887431973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>875071561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0        1        0       4  878542960\n",
       "1        1        1       3  876893119\n",
       "2        1        2       3  889751712\n",
       "3        1        3       5  887431973\n",
       "4        1        4       4  875071561"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=movielens['train_set']\n",
    "test=movielens['test_set']\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item_id\n",
       "36     461\n",
       "80     414\n",
       "207    409\n",
       "298    406\n",
       "251    402\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Mean rate movie model\n",
    "p = popularity_models('mostRated',train)\n",
    "p.train()\n",
    "p.model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 36,  80, 207])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.predict(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"b'Star Wars (1977)'\", \"b'Fargo (1996)'\", \"b'Contact (1997)'\"]\n"
     ]
    }
   ],
   "source": [
    "p.get_titles(p.predict(3), movielens.unique_item_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item_id\n",
       "36     252\n",
       "80     185\n",
       "105    175\n",
       "140    166\n",
       "252    150\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2 = popularity_models('positiveRated',train)\n",
    "p2.train(min_rating=5)\n",
    "p2.model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    \"\"\"\n",
    "    Implement the two most important relevance's metric for RS:\n",
    "        . Recall\n",
    "        . MAP\n",
    "    \"\"\"\n",
    "    def __init__(self, train_ids, test_ids, recommended_ids):\n",
    "        \n",
    "        self.zipped = list(zip(test_ids, train_ids, recommended_ids))\n",
    "        \n",
    "    def recall_per_user(self, N, test, recommended, train):\n",
    "        \"\"\"\n",
    "        :param N: number of recommendations\n",
    "        :param test: list of movies seen by user in test\n",
    "        :param recommended: list of movies recommended\n",
    "\n",
    "        :return the recall\n",
    "        \"\"\"   \n",
    "        if train is not None: \n",
    "            rec_true = []\n",
    "            for r in recommended:\n",
    "                if r not in train:\n",
    "                    rec_true.append(r)\n",
    "            else:\n",
    "                rec_true = recommended   \n",
    "        intersection = len(set(test) & set(rec_true[:N]))\n",
    "        return intersection / float(np.minimum(N, len(test)))\n",
    "       \n",
    "    def recall_at_n(self, topN):\n",
    "        \"\"\"\n",
    "        Computes the mean recall@k, for k in topN\n",
    "        :param topN: list of positions\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        for k in topN:\n",
    "            recall = np.mean([self.recall_per_user(k, test, recom, train)  for (test, train, recom) in self.zipped])\n",
    "            out.append(recall)\n",
    "            print(\"recall@%s=%.3f\" %(k, recall))\n",
    "        return out\n",
    "                                       \n",
    "    def apk(self, N, test, recommended, train):\n",
    "        \"\"\"\n",
    "        Computes the average precision at N given recommendations.\n",
    "\n",
    "        :param N: number of recommendations\n",
    "        :param test: list of movies seen by user in test\n",
    "        :param recommended: list of movies recommended\n",
    "\n",
    "        :return The average precision at N over the test set\n",
    "        \"\"\"   \n",
    "        if train is not None: \n",
    "            rec_true = []\n",
    "            for r in recommended:\n",
    "                if r not in train:\n",
    "                    rec_true.append(r)\n",
    "        else:\n",
    "            rec_true = recommended    \n",
    "        predicted = rec_true[:N] # top-k predictions\n",
    "\n",
    "        score = 0.0 # This will store the numerator\n",
    "        num_hits = 0.0 # This will store the sum of rel(i)\n",
    "\n",
    "        for i,p in enumerate(predicted):\n",
    "            if p in test and p not in predicted[:i]:\n",
    "                num_hits += 1.0\n",
    "                score += num_hits/(i+1.0)\n",
    "\n",
    "        return score / min(len(test), N)\n",
    "\n",
    "    def map_at_n(self, topN):\n",
    "        \"\"\"\n",
    "        Computes the mean average precision at k, for k in topN\n",
    "        :param topN: list of positions\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        for k in topN:\n",
    "            map_ = np.mean([self.apk(k, test, recom, train)  for (test, train, recom) in self.zipped])\n",
    "            out.append(map_)\n",
    "            print(\"map@%s=%.3f\" %(k, map_))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "topk = [5, 10, 20, 50]\n",
    "trainUsersGrouped = movielens['train_set'].groupby('user_id')['item_id'].apply(list).reset_index().sort_index()\n",
    "train_ids = trainUsersGrouped.item_id.values\n",
    "testUsersGrouped = movielens['test_set'].groupby('user_id')['item_id'].apply(list).reset_index().sort_index()\n",
    "test_ids = testUsersGrouped.item_id.values\n",
    "\n",
    "predictions = trainUsersGrouped.item_id.apply(lambda x: p.predict(np.max(topk))).reset_index()\n",
    "predictions_ids = predictions.item_id.values\n",
    "m = Metrics(train_ids, test_ids, predictions_ids)\n",
    "print('*****Recall*****')\n",
    "_ = m.recall_at_n(topk)\n",
    "print('*****Map*****')\n",
    "_ = m.map_at_n(topk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://courses.edx.org/c4x/BerkeleyX/CS100.1x/asset/Collaborative_filtering.gif\" alt=\"collaborative filtering\" style=\"float: right; width: 300px\"/>\n",
    "\n",
    "## 4. Collaborative Filtering <a id='cf'></a>\n",
    "\n",
    "Perhaps, one of the most succesful techniques for making personalized recommendations are the so called *collaborative filtering* (CF) algorithms. CF is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue X than to have the opinion on X of a person chosen randomly. \n",
    "\n",
    "The image at the right (from Wikipedia) shows an example of user's preference prediction using collaborative filtering. At first, people rate different items (like videos, images, games). After that, the system is making predictions about a user's rating for an item, which the user has not rated yet. These predictions are built upon the existing ratings of other users, who have similar ratings with the active user. For instance, in the image at the right the system has made a prediction, that the active user will not like the video.\n",
    "\n",
    "In this part we will see three kinds of CF, of increasing complexity:\n",
    "\n",
    "4.1 [CF with co-occurrence](#copurchase)\n",
    "\n",
    "4.2 [Memory-based CF](#memory-base)\n",
    "\n",
    "4.3 [Model-based CF](#model-base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='copurchase'></a>\n",
    "## 4.1 Co-occurrence Matrix\n",
    "\n",
    "The idea is to recommend movies similar to the movies already seen by a user. A measurement of similarity among items is obtained from the co-occurrence matrix. This is nothing else than the adjacency matrix of the graph of items created by users!!!\n",
    "\n",
    "<table border=\"0\" style=\"width:825px;border:0px;\">\n",
    "<tr>\n",
    "    <td> \n",
    "        <img src=\"https://lucidworks.com/wp-content/uploads/2015/08/Les-Miserables-Co-Occurrence.png\" style=\"width: 500px\"/>\n",
    "    </td>\n",
    "    <td> \n",
    "        <img src=\"https://lucidworks.com/wp-content/uploads/2015/08/midnight-club-graph.png\" style=\"width: 400px\"/>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Define the co-ocurrance matrix, which have shape=[n_items,n_items]. Filter movies rated with rating >=4.0\n",
    "\n",
    "Suggested steps:\n",
    "* create a dictionary of movies per user: moviesPerUser ={user: array[movies]}\n",
    "* co-ocurrance matrix: coMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict of user_id: np.array(movie_id) \n",
    "moviesPerUser = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the co-occurrance matrix\n",
    "coMatrix = np.zeros((movielens.n_items, movielens.n_items))\n",
    "for movies in moviesPerUser.items():\n",
    "    ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the matrix with the following piece of code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(coMatrix, fignum=1000, cmap=plt.cm.binary)\n",
    "plt.gcf().set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "**QUESTION:** Can you think of a better way of visualizaing this matrix? Try to rescale it, or to rearrenge it follwoing some criteria (for instance, popularity!).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "popular_indexing = ?\n",
    "coMatrix_sorted = ?\n",
    "coMatrix_sorted_total = ?\n",
    "log_scale = np.log(coMatrix_sorted_total+1.0)\n",
    "plt.matshow(log_scale, fignum=1000, cmap=plt.cm.binary)\n",
    "plt.gcf().set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Making predictions using the co-occurrence matrix\n",
    "\n",
    "This kind of recommendations, based on item similarity, provide a measure of the closeness of one item to another. In order to make a recommendation for a user, we have to proceed as follows:\n",
    "\n",
    "* First, define a function that returns the top-N closest items to a given one.\n",
    "* Then, for a list of items adopted by a specific user, select the top-N items from the lists of top-N closest items to each adopted item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurrance_similarity(item_id, coocurrance, ntop=10):\n",
    "    \"\"\"\n",
    "    Returns the top-N most similar items to a given one, based on the coocurrance matrix\n",
    "    \n",
    "    :param item_id: id of input item\n",
    "    :param cooccurrance: 2-dim numpy array with the co-occurance matrix\n",
    "    :param ntop: number of items to be retrieved\n",
    "    \n",
    "    :return top-N most similar items to the given item_id\n",
    "    \"\"\"\n",
    "    similarItems = ?\n",
    "    # return indeces of most similar items in descendign order: use numpy argsort\n",
    "    mostSimilar = ?\n",
    "    # remove the first element, as it is the item itslef\n",
    "    mostSimilar = ?\n",
    "    \n",
    "    # return a numpy array with the index (first column) and the value (second column) of the most similar items\n",
    "    return np.stack((mostSimilar, similarItems[mostSimilar])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO. Run the co_occurrance_similarity method\n",
    "co_occurrance_similarity(49, coMatrix, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Play with the movie ID, and print out its first Ntop recommendations!!\n",
    "queryMovieId = 140\n",
    "Ntop = 5\n",
    "print('For item \"%s\" top-%s recommendations are:' % (movielens.unique_item_dict[queryMovieId], Ntop))\n",
    "\n",
    "similarItems = co_occurrance_similarity(queryMovieId, coMatrix, Ntop)\n",
    "# let's print out the first Ntop recommendations\n",
    "for r in similarItems:\n",
    "    print(int(r[0]), movielens.unique_item_dict[int(r[0])], r[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let use this function to make recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "def co_occurrance_recommendation(items_id, cooccurrance, ntop=10):\n",
    "    \"\"\"\n",
    "    Obtain the list of ntop recommendations based on a list of items (user history of views)\n",
    "    \n",
    "    :param items_id: list of items ids\n",
    "    :param coocurrence: co-ocurrence matrix (numpy 2-dim array)\n",
    "    :param ntop: top-K items to be retrieved\n",
    "    \n",
    "    :return list of ntop items recommended\n",
    "    \"\"\"\n",
    "    # another (row wise)\n",
    "    list_sim_items = np.vstack([co_occurrance_similarity(id_, cooccurrance, ntop) for id_ in items_id])\n",
    "    # Group by id and take the maximum frquency to remove duplicates\n",
    "    largest_freq = pd.DataFrame(list_sim_items, columns=['id', 'freq']).groupby('id').agg(max).reset_index()\n",
    "    \n",
    "    # sort by value in descending order\n",
    "    sorted_list = largest_freq.sort_values(by='freq', ascending=False)\n",
    "    \n",
    "    # get the top N\n",
    "    out = sorted_list.values[:ntop, 0]\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the predictions (aka, recommendations)\n",
    "\n",
    "Note that the predictions will be calculated for Ntop items. If you want the predictions for all the items, just set:\n",
    "\n",
    "Ntop = movielens.n_items\n",
    "\n",
    "Otherwise, choose your value. For instance:\n",
    "\n",
    "Ntop = 200\n",
    "\n",
    "The latter will be way faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ntop = movielens.n_items\n",
    "Ntop = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the dataframe trainUsersGrouped and the method co_occurrance_recommendation\n",
    "predictions = ?\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, unlike previous popularity based models, the recommendation is now (slightly) different from one user to another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evalute the recommendation\n",
    "\n",
    "Get the `recall_at_n` for several n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = [5, 10, 20, 50]\n",
    "predictions_ids = predictions.item_id.values\n",
    "m = Metrics(train_ids, test_ids, predictions_ids)\n",
    "_ = m.recall_at_n(topk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Do the same analysis for the map metric*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = m.map_at_n(topk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "Compare this results to those obtained with the popularity model. Was it so bad?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some comments:\n",
    "* The dataset we are using here is very simple. Indeed, you can see that the popularity baseline achieves quite decent metric values. This won't happen in a real world dataset! The reason it happens here is because the dataset is quite small, and quite biased towards popular items. \n",
    "\n",
    "* Recall does not account for the order of the recommendation, while map does. This explains why the co-occurrance model performs better after the first 10 recommendations in terms of map (ordering), while recall values are always better for the popularity based model.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Oher distances\n",
    "\n",
    "So far, we have defined the *closeness* of two items as the number of users shared. However, it would make make sense to define it relative the total number of users that have watch a movie. This can be done with the [Jaccard similarity index](https://en.wikipedia.org/wiki/Jaccard_index):\n",
    "\n",
    "$$J(i,j)=\\frac{|i\\cap j|}{|i|+|j|-|i\\cap j|}\\in [0,1]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "Build the Jaccard similarity matrix from the co-occurrance matrix. Notice that $CoM(i,j) = |i\\cap j|$ and $CoM(k,k) = |k|$. In addition, if both $|i|=0$ and $|j|=0$, the similarity is defined as 1 (this is a convention).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Define the Jaccard similarity matrix: jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard = np.zeros((movielens.n_items, movielens.n_items)) # Jaccard similarity matrix\n",
    "for i, row in enumerate(coMatrix):\n",
    "    if row[i]!=0: # Case where the diagonal is not empty, i.e. coM(i,i)!=0\n",
    "        jaccard[i,:] = ?\n",
    "    else: # case where the diagonal is empty. We have to aasign a similarity of 1 to item pairs without ratings\n",
    "        for j in np.arange(movielens.n_items):\n",
    "            if coMatrix[j,j]==0:\n",
    "                jaccard[i,j] = ?\n",
    "            else: \n",
    "                jaccard[i,j] = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let's visualize the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(jaccard, fignum=1000, cmap=plt.cm.binary)\n",
    "plt.gcf().set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rearrange following popularity criteria (avoid ones!):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity = popularity_models('mostRated', movielens['train_set'])\n",
    "popularity.train()\n",
    "popular_indexing = popularity.model.index.values.astype(np.int)\n",
    "jaccard_sorted = jaccard[:,popular_indexing]\n",
    "jaccard_sorted_total = jaccard_sorted[popular_indexing, :]\n",
    "\n",
    "# Remove ones:\n",
    "jaccard_sorted_total[jaccard_sorted_total == 1.0] = 0.0\n",
    "cax = plt.matshow(jaccard_sorted_total, fignum=1000, cmap=plt.cm.coolwarm)\n",
    "plt.gcf().colorbar(cax, ticks=[0, 0.1, 0.2, 0.25])\n",
    "plt.clim(0, 0.25)\n",
    "plt.gcf().set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Recommend using Jaccard similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntop = 200\n",
    "# Calculate the predictoins with Jaccard\n",
    "predictions = ?\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = [5, 10, 20, 50, 100]\n",
    "predictions_ids = predictions.item_id.values\n",
    "m = Metrics(train_ids, test_ids, predictions_ids)\n",
    "_ = m.recall_at_n(topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = m.map_at_n(topk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='memory-base'></a>\n",
    "## 4.2. Memory-Based Collaborative Filtering (CF)\n",
    "\n",
    "Although the methods developed so far return a list of recommended items, they cannot be used to make an actual prediction regarding the rating. A quite different approach would be to calculate the unknown rating, $r_{ui}$, as the averaged of some other ratings, thta are somehow close to either the user or the item in question. \n",
    "\n",
    "Thus, one approach is to take\n",
    "\n",
    "### $$r_{u,i} = \\frac{1}{K}\\sum_{j\\in\\mathcal{I}'} \\mathrm{sim}(i,j) r_{u,j},$$\n",
    "\n",
    "where items $j\\in\\mathcal{I}'$ are taken from the set of $K$ closest items to $i$, or from the whole dataset. This is known as **item-item collaborative filtering**, and can be interpreted as *“users who liked this movie also liked …”*. See Amazon famous patent: https://www.google.com/patents/US7113917. Basically, this technique will take an item, find users who liked that item, and find other items that those users or similar users also liked. \n",
    "\n",
    "Similarly, one can define a **user-user filtering** where predictions are made as\n",
    "\n",
    "### $$r_{u,i} = \\frac{1}{K} \\sum_{v\\in\\mathcal{U}'} \\mathrm{sim}(u,v) r_{v,i}.$$\n",
    "\n",
    "<img src=\"https://soundsuggest.files.wordpress.com/2013/06/utility_matrix.png\" alt=\"utility matrix\" style=\"float: right; width: 400px\"/>\n",
    "\n",
    "In this case, the recommendation would be more like *“users who are similar to you also liked …”*. Both techniques are part of the broad familiy of **Memory-Based Collaborative Filtering** approaches, or neighborhood-based algorithms.\n",
    "\n",
    "The similarity among users or items can be calculated in a variety of forms: Pearson's correlation, cosine distance, etc. Here we will use the cosine distance. For this, we will first create the utility user-item matrix. \n",
    "\n",
    "The utility matrix is a dense representation of the user-item intearction. We have been using the *long* format, where missing entries are obviated; now, we will use the *wide* format, i.e. the matrix representation (see the figure on the right). \n",
    "\n",
    "<br></br>\n",
    "<div class = \"alert alert-info\">\n",
    "** NOTE **: Long and wide formats have its benefits and drawbacks. Can you think of some of them?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the train and test datasets in wide format (i.e., like a matrix). \n",
    "\n",
    "Name this variables uMatrixTraining and uMatrixTesting. \n",
    "\n",
    "Important!! Note user ids start at 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uMatrixTraining = np.zeros((movielens.n_users, movielens.n_items)) # utility matrix\n",
    "for row in movielens['train_set'].values[:,0:3]:\n",
    "    # Note user ids start at 1\n",
    "    ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uMatrixTesting = np.zeros((movielens.n_users, movielens.n_items)) # utility matrix\n",
    "for row in movielens['test_set'].values[:,0:3]:\n",
    "    # Note user ids start at 1\n",
    "    ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a similarity measure: cosine similarity\n",
    "\n",
    "#### $$\\mathrm{sim}({\\bf a},{\\bf b})=\\frac{{\\bf a}\\cdot{\\bf b}}{\\sqrt{{\\bf a}\\cdot{\\bf a}}\\sqrt{{\\bf  b}\\cdot{\\bf b}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "def cosineSimilarity(ratings, kind='user', epsilon=1e-9):\n",
    "    \"\"\"\n",
    "    Calculate the cosine distance along the row (columns) of a matrix for users (items)\n",
    "    \n",
    "    :param ratings: a n_user X n_items matrix\n",
    "    :param kind: string indicating whether we are in mode 'user' or 'item'\n",
    "    :param epsilon: a small value to avoid dividing by zero (optional, defaults to 1e-9)\n",
    "    \n",
    "    :return a square matrix with the similarities\n",
    "    \"\"\"\n",
    "    # epsilon -> small number for handling dived-by-zero errors\n",
    "    if kind == 'user':\n",
    "        sim = ?\n",
    "    elif kind == 'item':\n",
    "        sim = ?\n",
    "    norms = np.array([np.sqrt(np.diagonal(sim))])\n",
    "    return sim / norms / norms.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. User-user CF\n",
    "\n",
    "*“Users who are similar to you also liked …”*\n",
    "\n",
    "### $$r_{u,i} = \\frac{1}{K} \\sum_{v\\in\\mathcal{U}'} \\mathrm{sim}(u,v) r_{v,i}.$$\n",
    "Consider user $x$:\n",
    "\n",
    "1. Find other users whose ratings are “similar” to $x$’s ratings, i.e. calculate the similarity among users with the help of the cosineSimilarity function.\n",
    "2. Estimate missing ratings based on ratings of similar users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userSimilarity = cosineSimilarity(uMatrixTraining, kind='user')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the userSimilarity matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cax = plt.matshow(userSimilarity, fignum=1000, cmap=plt.cm.coolwarm)\n",
    "plt.gcf().colorbar(cax, ticks=[0, 0.25, 0.5])\n",
    "plt.clim(0, 0.5)\n",
    "plt.gcf().set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, get userItemCFpredictions\n",
    "\n",
    "*Note that if we multiply `userSimilarity` by `uMatrixTraining` we get the ratings weigthed with user similar similarity. Then, we have to normalize by the average similarity for each user*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = np.array([userSimilarity.sum(axis=1)]).T\n",
    "userItemCFpredictions = userSimilarity.dot(uMatrixTraining) / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that some users might give generally lower ratings than others, so that we could have also corrected for this effect as follows*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_ = uMatrixTraining.sum(axis=1)\n",
    "len_ =np.count_nonzero(uMatrixTraining, axis=1)\n",
    "average_ratings = np.tile(sum_/ len_, movielens.n_items).reshape([movielens.n_items, movielens.n_users]).T\n",
    "uMatrixTraining_shifted = uMatrixTraining - np.multiply(average_ratings, uMatrixTraining)\n",
    "userItemCFpredictions_corrected = average_ratings + userSimilarity.dot(uMatrixTraining_shifted) / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Item-Item CF\n",
    "\n",
    "*“Users who liked this movie also liked …”*\n",
    "\n",
    "Consider item $i$, and following the previous section:\n",
    "\n",
    "1. For item $i$, find other similar items, i.e. calculate the similarity itemSimilarity among items as before we did for users\n",
    "2. Estimate rating for item $i$ based on ratings for similar items: itemItemCFpredictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemSimilarity = cosineSimilarity(uMatrixTraining, kind='item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemItemCFpredictions = uMatrixTraining.dot(itemSimilarity)/np.array([np.abs(itemSimilarity).sum(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Show some recommendations\n",
    "\n",
    "In case of item-item CF, the recommendation is pretty much the same as with the co-occurence matrix. It's also quite simple to find similar items to a given one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Find movies similar to a given one using the item-item similarity matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryMovieId = 720\n",
    "print(\"Selected item is '%s'\" % movielens.unique_item_dict[queryMovieId])\n",
    "\n",
    "\n",
    "queryAnswer = itemSimilarity[queryMovieId,:]\n",
    "queryAnswer = np.argsort(queryAnswer)[::-1] #descending order\n",
    "queryAnswer = queryAnswer[1:]  # remove first item (itself)\n",
    "\n",
    "# let's print out the most similar items\n",
    "Ntop = 10\n",
    "print(\"Most %d similar movies are:\" %Ntop)\n",
    "printAnswer = queryAnswer[0:Ntop]\n",
    "for answerId in printAnswer:\n",
    "    print(movielens.unique_item_dict[answerId]+\", with similarity %.2f\" %itemSimilarity[queryMovieId, answerId])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Calculate the recommendations obtained with the item-item CF model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemItemCFpredictions[uMatrixTraining>=4.0] = 0.0\n",
    "for u in np.random.randint(0, movielens.n_users, 3):\n",
    "    print(\"*\"*6)\n",
    "    print(\"User %s\" % u)\n",
    "    print(\"Seen items: \")\n",
    "    seen = uMatrixTesting[u,:]\n",
    "    print([movielens.unique_item_dict[i] for i,r in enumerate(seen) if r>4.0])\n",
    "    print(\"Recommended items: \")\n",
    "    recom = itemItemCFpredictions[u,:]\n",
    "    recom = np.argsort(recom)[::-1][:10]\n",
    "    print([movielens.unique_item_dict[i] for i in recom])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Do the same with the user-user CF model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userItemCFpredictions[uMatrixTraining>=4.0] = 0.0\n",
    "userItemCFpredictions_corrected[uMatrixTraining>=4.0] = 0.0\n",
    "for u in np.random.randint(0, movielens.n_users, 3):\n",
    "    print(\"*\"*6)\n",
    "    print(\"User %s\" % u)\n",
    "    print(\"Seen items: \")\n",
    "    seen = uMatrixTesting[u,:]\n",
    "    print([movielens.unique_item_dict[i] for i,r in enumerate(seen) if r>4.0])\n",
    "    print(\"Recommended items: \")\n",
    "    recom = userItemCFpredictions[u,:]\n",
    "    recom = np.argsort(recom)[::-1][:10]\n",
    "    print([movielens.unique_item_dict[i] for i in recom])\n",
    "    print(\"Recommended items (shifted version): \")\n",
    "    recom = userItemCFpredictions_corrected[u,:]\n",
    "    recom = np.argsort(recom)[::-1][:10]\n",
    "    print([movielens.unique_item_dict[i] for i in recom])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you can see, the recommendations are pretty bad... Let's measure that**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 Measure the recommendations\n",
    "\n",
    "Since we are predicting ratings, it might make sense to introduce a metric that accounts for this. In particular, the **Root Mean Square Error (RMSE)** is typically used for this purpose. \n",
    "\n",
    "### $$\\mathrm{RMSE}=\\sqrt{\\frac{1}{n_{\\mathrm{users}}n_{\\mathrm{items}}}\\sum_{u,i}\\left(r_{u,i}-\\hat{r}_{u,i}\\right)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "from math import sqrt\n",
    "def rmse(prediction, ground_truth):\n",
    "    \"\"\"\n",
    "    Return the Root Mean Squared Error of the prediction\n",
    "    \n",
    "    :param prediction: a 2-dim numpy array with the predictions\n",
    "    :param ground_truth: a 2-dim numpy array with the known ratings\n",
    "    \n",
    "    :return the RMSE\n",
    "    \"\"\"\n",
    "    # get indices of non-zero elements at test\n",
    "    r, c = ground_truth.nonzero()\n",
    "    # get non-zero elements at prediction\n",
    "    p = prediction[r,c]\n",
    "    # get non-zero elements at test\n",
    "    t = ground_truth[r,c]\n",
    "    return sqrt(np.mean(np.power(p-t, 2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('User-based CF RMSE=%.3f' %rmse(userItemCFpredictions, uMatrixTesting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Item-based CF RMSE=%.3f' %rmse(itemItemCFpredictions, uMatrixTesting))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate ranking metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the recommendations\n",
    "userItemCFpredictions_sorted = np.argsort(userItemCFpredictions)[::-1]\n",
    "# get the first 200, to speed up things\n",
    "userItemCFpredictions_sorted = userItemCFpredictions_sorted[:, :200]\n",
    "\n",
    "test_ids = testUsersGrouped.item_id.values\n",
    "train_ids = trainUsersGrouped.item_id.values\n",
    "predicted_ids = trainUsersGrouped.user_id.apply(lambda i: userItemCFpredictions_sorted[i-1]).values\n",
    "topk = [5, 10, 20, 50, 100]\n",
    "m = Metrics(train_ids, test_ids, predicted_ids)\n",
    "print('*****Recall*****')\n",
    "_ = m.recall_at_n(topk)\n",
    "print('*****Map*****')\n",
    "_ = m.map_at_n(topk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-base'></a>\n",
    "## 4.3. Model-based CF or Latent factor models\n",
    "There are several model-based CF: from matrix factorizations to bayesian models, neural netwroks, etc. In all of them, we try to extract latent factors (vectors) that model user and item interactions. In contrast to previous methods, our hypothesis here is that the dimension of the latent spaces is rather small (in the order of a hundred dimensions). Then, we use this latent features to make a prediction:\n",
    "\n",
    "## $$r_{u,i} \\approx {\\bf f}_u^T\\cdot{\\bf f}_i$$\n",
    "\n",
    "The underlying assumption is that both users and items *live* in the same latent space, and that we can unravel such space. \n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Tunca_Dogan/publication/235913413/figure/fig3/AS:299678856957952@1448460415040/Figure-3-The-distribution-of-the-points-in-the-Swiss-roll-dataset-in-3-D-space.png\n",
    "\" alt=\"swiss roll\" style=\"float: center; width: 300px\"/>\n",
    "\n",
    "\n",
    "Here we will use a couple of linear Matrix Factorization (MF) models:\n",
    "\n",
    "* Singular Value decomposition (SVD)\n",
    "* Alternating Least Squares (ALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Singular value decomposition\n",
    "\n",
    "The main idea is to reduce the dimensionality of the input space. This is pretty much the same as Eigen-decomposition or Principal Component Analysis (PCA)-\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/220px-GaussianScatterPCA.svg.png\n",
    "\" alt=\"dimensionaly reducion\" style=\"float: center; width: 500px\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Get SVD components from train matrix. Choose k=20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=20\n",
    "u, s, vt = svds(uMatrixTraining, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Take a look at the different matrices\n",
    "* check the shapes\n",
    "* check if U and V are orthogonal matrices with the left singular vectors as columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u.shape)\n",
    "# Check U is orthogonal\n",
    "print(rmse(np.dot(u.T,u), np.identity(k)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same with V\n",
    "print(vt.shape)\n",
    "print(rmse(np.dot(vt,vt.T), np.identity(k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the recommendations\n",
    "\n",
    "We will reconstruct the utility matrix R (i.e., the recommendation matrix) as follows:\n",
    "\n",
    "### $$M\\approx U\\mathrm{diag}(s)V^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, build a diagonal matrix s_diag_matrix with the eigenvalues\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_diag_matrix = np.diag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the prediction: svdPredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svdPredictions = np.dot(np.dot(u, s_diag_matrix), vt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the dimensions of svdPredictions and uMatrixTesting are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svdPredictions.shape)\n",
    "print(uMatrixTesting.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "\n",
    "* Recall@{10,100}\n",
    "* Play with more dimensions: svd_dim in {10, 50, 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVD RMSE=%.3f' % rmse(svdPredictions, uMatrixTesting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall\n",
    "svdPredictions_sorted = np.argsort(svdPredictions)[::-1]\n",
    "svdPredictions_sorted = svdPredictions_sorted[:, :200]\n",
    "test_ids = testUsersGrouped.item_id.values\n",
    "predicted_ids = trainUsersGrouped.user_id.apply(lambda i: svdPredictions_sorted[i-1]).values\n",
    "topk = [10, 20, 50, 100]\n",
    "m = Metrics(train_ids, test_ids, predicted_ids)\n",
    "print('*****Recall*****')\n",
    "_ = m.recall_at_n(topk)\n",
    "print('*****Map*****')\n",
    "_ = m.map_at_n(topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in [10, 50, 100]:\n",
    "    print(\"*\"*30)\n",
    "    print(\"Using %s latent dimensions\" %dim)\n",
    "    # apply Singular Value Decomposition\n",
    "    u, s, vt = svds(uMatrixTraining, dim)\n",
    "    s_diag_matrix = np.diag(s)\n",
    "    # make the prediction\n",
    "    svdPredictions = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "    print('SVD RMSE=%.3f' % rmse(svdPredictions, uMatrixTesting))\n",
    "    # recall\n",
    "    svdPredictions_sorted = np.argsort(svdPredictions)[::-1]\n",
    "    svdPredictions_sorted = svdPredictions_sorted[:, :200]\n",
    "    \n",
    "    test_ids = testUsersGrouped.item_id.values\n",
    "    predicted_ids = trainUsersGrouped.user_id.apply(lambda i: svdPredictions_sorted[i-1]).values\n",
    "    topk = [5, 10, 20, 50, 100]\n",
    "    m = Metrics(train_ids, test_ids, predicted_ids)\n",
    "    print('*****Recall*****')\n",
    "    _ = m.recall_at_n(topk)\n",
    "    print('*****Map*****')\n",
    "    _ = m.map_at_n(topk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implicit vs Explicit feedback\n",
    "\n",
    "In the above SVD matrix factorization we have tried to reconstructt the matrix of ratings. However, it might be easier trying to model the matrix of preferences (i.e., wether the user likes or not a movie).\n",
    "\n",
    "For this, we will define a “selector” matrix $I$ for the training utility matrix $R$, which will contain 0 if the rating matrix has no rating entry, and 1 if the rating matrix contains an entry. And the smae for testing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert uMatrixTraining and uMatrixTesting into implicit data (I and I2 respectively). For these new matrices, if r>3 then 1, else = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index matrix for training data\n",
    "I = uMatrixTraining.copy()\n",
    "I[I > 3] = 1\n",
    "I[I == 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index matrix for test data\n",
    "I2 = uMatrixTesting.copy()\n",
    "I2[I2 > 3] = 1\n",
    "I2[I2 == 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Evaluate SVD with RMSE, and recall@k and map@k, with k in [10, 30, 50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in [50]:\n",
    "    print(\"*\"*30)\n",
    "    print(\"Using %s latent dimensions\" %dim)\n",
    "    # apply Singular Value Decomposition\n",
    "    u, s, vt = svds(I, dim)\n",
    "    s_diag_matrix = np.diag(s)\n",
    "    # make the prediction\n",
    "    svdPredictions = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "    print('SVD RMSE=%.3f' % rmse(svdPredictions, I2))\n",
    "    # recall\n",
    "    svdPredictions_sorted = np.argsort(svdPredictions)[::-1]\n",
    "    svdPredictions_sorted = svdPredictions_sorted[:, :200]\n",
    "    test_ids = testUsersGrouped.item_id.values\n",
    "    predicted_ids = trainUsersGrouped.user_id.apply(lambda i: svdPredictions_sorted[i-1]).values\n",
    "    topk = [10, 50, 100]\n",
    "    m = Metrics(train_ids, test_ids, predicted_ids)\n",
    "    print('*****Recall*****')\n",
    "    _ = m.recall_at_n(topk)\n",
    "    print('*****Map*****')\n",
    "    _ = m.map_at_n(topk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "As we introduce more dimensions in the model, we make it more prone to overfit. This can be observed in the decrease of error (RMSE) in train, while it increases in test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_train = []\n",
    "error_test = []\n",
    "dims = [2, 5, 10, 20, 40, 80, 200]\n",
    "for dim in dims:\n",
    "    print(\"*\"*30)\n",
    "    print(\"Using %s latent dimensions\" %dim)\n",
    "    # apply Singular Value Decomposition\n",
    "    u, s, vt = svds(I, dim)\n",
    "    s_diag_matrix = np.diag(s)\n",
    "    # make the prediction\n",
    "    svdPredictions = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "    error_train.append(rmse(svdPredictions, I))\n",
    "    error_test.append(rmse(svdPredictions, I2)) \n",
    "plt.semilogx(dims, error_train, '--*b', label=\"train\")\n",
    "plt.semilogx(dims, error_test, '--*g', label=\"test\")\n",
    "plt.xlabel(\"Numer of latent dimensions\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend(loc=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that after few dimensions (~10) the model starts to overfit (the error in test increases). Thus, we need to use other method to regularize the model (i.e., set restrictions/limitations to the model, so that it cannot overfit)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Alternating Least Squares (ALS)\n",
    "\n",
    "SVD can be very slow and computationally expensive. Besides, when addressing only the relatively few known entries we are highly prone to overfitting.\n",
    "\n",
    "An scalable alternative to SVD is ALS, which can include regularization terms to prevent overfitting. We will rename our variable to make them more similar to the ALS notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS algorithm\n",
    "\n",
    "The ALS algorithm aims to estimate two unknown matrices which, when multiplied together, yield the rating matrix. The loss function you will use is the well-known sum of squared errors. The second term is for regularisation to prevent overfitting\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\underset{Q*&space;,&space;P*}{min}\\sum_{(u,i)\\epsilon&space;K&space;}(r_{ui}-P_u^TQ_i)^2&plus;\\lambda(\\left&space;\\|&space;Q_i&space;\\right&space;\\|^2&space;&plus;&space;\\left&space;\\|&space;P_u&space;\\right&space;\\|^2)$&space;&space;$\" title=\"\\underset{q* , p*}{min}\\sum_{(u,i)\\epsilon K }(r_{ui}-q_i^Tp_u)^2+\\lambda(\\left \\| q_i \\right \\|^2 + \\left \\| p_u \\right \\|^2)\" />\n",
    "\n",
    "The Alternating Least Squares algorithm does this by first randomly filling the users matrix with values and then optimizing the value of the movies such that the error is minimized.  Then, it holds the movies matrix constant and optimizes the value of the user's matrix.  This alternation between which matrix to optimize is the reason for the \"alternating\" in the name. \n",
    "\n",
    "<img alt=\"factorization\" src=\"http://spark-mooc.github.io/web-assets/images/matrix_factorization.png\" style=\"width: 885px\"/>\n",
    "<br clear=\"all\"/>\n",
    "\n",
    "This optimization is what's being shown on the right in the image above.  Given a fixed set of user factors (i.e., values in the users matrix), we use the known ratings to find the best values for the movie factors using the optimization written at the bottom of the figure.  Then we \"alternate\" and pick the best user factors given fixed movie factors.\n",
    "\n",
    "It must be noticed that this is another way of reducing the dimensionality of the input matrix (like PCA, or more generally, SVD). This has important consequences:\n",
    "\n",
    "* ### Our decomposition is linear. We won't be able to catch non-linear relationships among users and items.\n",
    "* ### As in PCA or SVD, our features will correspond to directions of maximum variance in the data. Thus, the first feature will catch most of this variation, the second, a little bit more, and so on. It implies that the error in the reconstruction will not decrease dramatically when using more features!!! Keep this in mind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alsRmse(I,R,P,Q):\n",
    "    return np.sqrt(np.sum((I * (R - np.dot(P,Q.T)))**2)/len(R[R > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def iteration(user, fixed_vecs, counts, num_factors, reg_param, num_solve, verbose=False):\n",
    "    num_fixed = fixed_vecs.shape[0]\n",
    "    YTY = fixed_vecs.T.dot(fixed_vecs)\n",
    "    eye = np.eye(num_fixed)\n",
    "    lambda_eye = reg_param * np.eye(num_factors)\n",
    "    solve_vecs = np.zeros((num_solve, num_factors))\n",
    "\n",
    "    t = time.time()\n",
    "    for i in range(num_solve):\n",
    "        if user:\n",
    "            counts_i = counts[i]\n",
    "        else:\n",
    "            counts_i = counts[:, i].T\n",
    "        CuI = np.diag(counts_i)\n",
    "        pu = counts_i.copy()\n",
    "        pu[np.where(pu != 0)] = 1.0\n",
    "        YTCuIY = fixed_vecs.T.dot(CuI).dot(fixed_vecs)\n",
    "        YTCupu = fixed_vecs.T.dot(CuI + eye).dot(pu.T)\n",
    "        xu = np.linalg.solve(YTY + YTCuIY + lambda_eye, YTCupu)\n",
    "        solve_vecs[i] = xu\n",
    "        if verbose and i % 300 == 0:\n",
    "            print('Solved %i vecs in %d seconds' % (i, time.time() - t))\n",
    "            t = time.time()\n",
    "\n",
    "    return solve_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check performance by plotting train and test errors\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def check_als_performance(n_epochs, train_errors, test_errors):\n",
    "    plt.plot(range(n_epochs), train_errors, marker='o', label='Training Data');\n",
    "    plt.plot(range(n_epochs), test_errors, marker='v', label='Test Data');\n",
    "    plt.title('ALS-WR Learning Curve')\n",
    "    plt.xlabel('Number of Epochs');\n",
    "    plt.ylabel('RMSE');\n",
    "    plt.ylim(1, 5)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 10\n",
    "num_factors = 50\n",
    "lmbda = 0.2  # this hyperparameter controlls the regularization. Play wiht it!\n",
    "T = uMatrixTesting.copy()\n",
    "ptest = uMatrixTesting.copy()\n",
    "ptest[ptest > 0] = 1\n",
    "ptest[ptest == 0] = 0\n",
    "p = uMatrixTraining.copy()\n",
    "p[p > 0] = 1\n",
    "p[p == 0] = 0\n",
    "R = uMatrixTraining.copy()\n",
    "alpha = movielens.n_pairs / (movielens.n_users * movielens.n_items)\n",
    "C = alpha * R\n",
    "user_vectors = np.random.normal(size=(movielens.n_users, num_factors))\n",
    "item_vectors = np.random.normal(size=(movielens.n_items, num_factors))\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "for i in range(num_iterations):\n",
    "    print('Solving for user vectors...')\n",
    "    user_vectors = iteration(True, item_vectors, C, num_factors, lmbda, movielens.n_users)\n",
    "    print('Solving for item vectors...')\n",
    "    item_vectors = iteration(False, user_vectors, C, num_factors, lmbda, movielens.n_items)\n",
    "    print('iteration %i finished' % (i + 1))\n",
    "    \n",
    "    train_rmse = alsRmse(p,uMatrixTraining, user_vectors,item_vectors)\n",
    "    test_rmse = alsRmse(ptest,uMatrixTesting, user_vectors,item_vectors)\n",
    "    train_errors.append(train_rmse)\n",
    "    test_errors.append(test_rmse)\n",
    "    print(\"[Epoch %d/%d] train error: %f, test error: %f\" \n",
    "        %(i+1, num_iterations, train_rmse, test_rmse))\n",
    "    \n",
    "print(\"Algorithm converged\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_als_performance(num_iterations, train_errors, test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS evaluation\n",
    "* RMSE\n",
    "* recall@\n",
    "* map@"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alsPredictions = np.dot(user_vectors, item_vectors.T)\n",
    "svdPredictions = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "print('ALS CF RMSE: ' + str(rmse(alsPredictions, uMatrixTesting)))\n",
    "print('SVD CF RMSE: ' + str(rmse(svdPredictions, uMatrixTesting)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalutation\n",
    "topk = [20, 50, 100]\n",
    "alsPredictions_sorted = np.argsort(alsPredictions)[::-1]\n",
    "alsPredictions_sorted = alsPredictions_sorted[:, :np.max(topk)]\n",
    "predicted_ids = trainUsersGrouped.user_id.apply(lambda i: alsPredictions_sorted[i-1]).values\n",
    "m = Metrics(train_ids, test_ids, predicted_ids)\n",
    "print('*****Recall*****')\n",
    "_ = m.recall_at_n(topk)\n",
    "print('*****Map*****')\n",
    "_ = m.map_at_n(topk)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
