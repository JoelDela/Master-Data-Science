{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# install the requirements\n",
    "# pip install torch torchvision\n",
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Modelos de Lenguaje de OpenAI\n",
    "\n",
    "A mitad de febrero, [OpenAI publicó un modelo de lenguaje](https://blog.openai.com/better-language-models/) capaz de generar lenguaje natural de formar coherente. Este modelo es generalista y, a pesar de ello, es capaz de rivalizar con los mejores sistemas específicos en tareas como comprensión automática de lenguaje natural, traducción automática, búsqueda de respuestas y resumen automático.\n",
    "\n",
    "Este modelo, llamado GPT-2, es el resultado de haber entrenado con 8 millones de páginas web (40 GB) con 1 500 millones de parámetros con un único objetivo: predecir cuál es la siguiente palabra.\n",
    "\n",
    "Sin embargo, OpenAI no ha publicado el modelo para evitar que alguien con malas intenciones pueda hacer un uso dañino de esta tecnología. Sí que han publicado una versión simplificada y más pequeña, y el paper [\"Language Models are Unsupervised Multitask Learners\"](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), en el que explican todo el proceso.\n",
    "\n",
    "Con ganas y GPUs suficientes (+ tiempo y dinero), se puede replicar el proceso. Otras lecturas interesantes, sobre el tema: \n",
    "\n",
    "- [OpenAI's new Multitalented AI Writes, Translates, and Slanders](https://www.theverge.com/2019/2/14/18224704/ai-machine-learning-language-models-read-write-openai-gpt2)\n",
    "- [Some thoughts on zero-day threats in AI, and OpenAI's GP2](https://www.fast.ai/2019/02/15/openai-gp2/)\n",
    "\n",
    "\n",
    "Este código de ejemplo está inspirado en [un tweet de Thomas Wolf](https://twitter.com/Thom_Wolf/status/1097465312579072000), de [Hugging Face](https://huggingface.co/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A continuación, definimos una función para:\n",
    "\n",
    "1. tokenizar el texto de entrada y codificarlo como un vector con los pesos obtenidos por el modelo GPT2\n",
    "2. predecir la siguiente palabra más frecuente\n",
    "3. decodificar el vector como una secuencia de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def generate(text, length=50):\n",
    "    \"\"\"Generate automatic Natural Language from the input text\"\"\"\n",
    "    vec_text = tokenizer.encode(text)\n",
    "    my_input, past = torch.tensor([vec_text]), None\n",
    "\n",
    "    for _ in range(length):\n",
    "        logits, past = model(my_input, past=past)\n",
    "        my_input = torch.multinomial(F.softmax(logits[:, -1], dim=1), 1)\n",
    "        vec_text.append(my_input.item())\n",
    "\n",
    "    return tokenizer.decode(vec_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The only think we can do to fight climate change is ask how we will cut greenhouse gas emissions without funding the civil service.\n",
      "\n",
      "If we don't; it is not our inalienable right to be Callous, Carbon \n",
      "\n",
      "The only think we can do to fight climate change is exploration...\" Chapter 13, No.10. Alcohol is, of course, often sold in vending machines across America. And it's hard to avoid Miller's claim that, when \n",
      "\n",
      "The only think we can do to fight climate change is to think about it. It's pretty difficult for citizens to understand what's behind it — and science tells us that more and more of us are dealing with it.\"\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# defino un texto de entrada\n",
    "text = \"The only think we can do to fight climate change is\"\n",
    "\n",
    "# y generamos automáticamente las secuencias más probables\n",
    "for _ in range(3):\n",
    "    print(generate(text, 35), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was born in Spain so I speak Spanish\n",
      "I was born in France so I speak French\n",
      "I was born in Italy so I speak Italian\n",
      "I was born in Greece so I speak Greek\n",
      "I was born in Russia so I speak with\n",
      "I was born in China so I speak Mandarin\n",
      "I was born in Japan so I speak Japanese\n",
      "I was born in India so I speak Hindi\n"
     ]
    }
   ],
   "source": [
    "countries = \"Spain France Italy Greece Russia China Japan India\".split()\n",
    "\n",
    "for country in countries:\n",
    "    text = f\"I was born in {country} so I speak\"\n",
    "    print(generate(text, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
